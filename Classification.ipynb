{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c98c9f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f0af9de",
   "metadata": {},
   "source": [
    "# Inport needed package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e2073d",
   "metadata": {},
   "source": [
    "- import os, sys # to add the parent directory to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6f3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0bf9d",
   "metadata": {},
   "source": [
    "- Using torchvision to create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84577080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using torchvision to create a dataset\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353412ad",
   "metadata": {},
   "source": [
    "- import self library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec759ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.trainer import ClassifierTrainer as Trainer\n",
    "import dataset as ds  # type: ignore\n",
    "import model as md  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec973ee2",
   "metadata": {},
   "source": [
    "# Define classification train process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872305b",
   "metadata": {},
   "source": [
    "1. Define place where the model is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65f0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d88245",
   "metadata": {},
   "source": [
    "2. Define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554ddc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTheTrain(dataset, model):\n",
    "  # define batch_size\n",
    "  batch_size = 64\n",
    "\n",
    "  # init train val test ds\n",
    "  train_val_size = int(0.8 * len(dataset))\n",
    "  test_size = len(dataset) - train_val_size\n",
    "  train_ds, test_ds = random_split(dataset, [train_val_size, test_size])\n",
    "\n",
    "  # define optimizer using Adam and loss function\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "  trainer = Trainer(model, optimizer, loss_fn, random_seed_value=86)\n",
    "  print('device: ', trainer.device)\n",
    "  avg_loss, metric = trainer.cross_validate(train_ds, k=5, epochs=10, batch_size=batch_size)\n",
    "  print('avg_loss: ', avg_loss)\n",
    "\n",
    "  # score model\n",
    "  test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "  model_scored = trainer.score(test_dataloader)\n",
    "  print(f'model_scored: {model_scored:.4f}, avg_accuracy: {100*(1 - model_scored):.4f}')\n",
    "\n",
    "  # return model scored, train_avg_lost\n",
    "  return model_scored, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e26c06",
   "metadata": {},
   "source": [
    "3. execute progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bbab88",
   "metadata": {},
   "source": [
    "- define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2317e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    md.embedded_resnet50(103),\n",
    "    md.embedded_densenet121(103),\n",
    "    md.embedded_vgg16(103)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5733d5",
   "metadata": {},
   "source": [
    "- Define tested datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a4d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'gi4e_full': ds.Gi4eDataset(\n",
    "        './datasets/gi4e',\n",
    "        transform=transforms.Compose([transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.ToTensor()]),\n",
    "        is_classification=True),\n",
    "    'gi4e_raw_eyes': ds.ImageDataset(\n",
    "        './datasets/gi4e_raw_eyes',\n",
    "        transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]),\n",
    "        file_extension='png'),\n",
    "    'gi4e_detected_eyes': ds.ImageDataset(\n",
    "        './datasets/gi4e_eyes/20250521_200316',\n",
    "        transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]),\n",
    "        file_extension='png'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28afcf22",
   "metadata": {},
   "source": [
    "- Train all defined model on each registered datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1784fdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gi4e_full dataset with Classifier\n",
      "device:  cuda\n",
      "Fold 1/5:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# do the train\u001b[39;00m\n\u001b[0;32m      7\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m scored, loss \u001b[38;5;241m=\u001b[39m \u001b[43mdoTheTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     10\u001b[0m total_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mdoTheTrain\u001b[1;34m(dataset, model)\u001b[0m\n\u001b[0;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optimizer, loss_fn, random_seed_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m86\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice: \u001b[39m\u001b[38;5;124m'\u001b[39m, trainer\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 16\u001b[0m avg_loss, metric \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_loss: \u001b[39m\u001b[38;5;124m'\u001b[39m, avg_loss)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# score model\u001b[39;00m\n",
      "File \u001b[1;32md:\\Workspace\\thesis_sources\\train\\trainer.py:73\u001b[0m, in \u001b[0;36mClassifierTrainer.cross_validate\u001b[1;34m(self, train_dataset, k, epochs, batch_size)\u001b[0m\n\u001b[0;32m     70\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     71\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m lost_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m report_metric\u001b[38;5;241m.\u001b[39mappend(lost_metric)\n\u001b[0;32m     75\u001b[0m fold_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(test_loader)\n",
      "File \u001b[1;32md:\\Workspace\\thesis_sources\\train\\trainer.py:130\u001b[0m, in \u001b[0;36mClassifierTrainer.train\u001b[1;34m(self, train_loader, test_loader, epochs)\u001b[0m\n\u001b[0;32m    128\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[0;32m    129\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(outputs, targets)\n\u001b[1;32m--> 130\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    132\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    332\u001b[0m     (inputs,)\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[0;32m    337\u001b[0m )\n\u001b[0;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:220\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    219\u001b[0m         new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 220\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         )\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(columns=['dataset', 'model', 'avg_loss', 'avg_accuracy', 'total_time'])\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "\tfor model in models:\n",
    "\t\tprint(f'Running {name} dataset with {model.__class__.__name__}')\n",
    "\t\t# do the train\n",
    "\t\tstart_time = time.time()\n",
    "\t\tscored, loss = doTheTrain(dataset, model)\n",
    "\t\tend_time = time.time()\n",
    "\t\ttotal_time = end_time - start_time\n",
    "\t\tprint(f'Finished {name} dataset with {model.__class__.__name__}')\n",
    "\t\tprint('----------------------')\n",
    "\n",
    "\t\t# save the result\n",
    "\t\tresult_df = pd.concat([result_df, pd.DataFrame({\n",
    "\t\t\t'model': [model.__class__.__name__],\n",
    "\t\t\t'dataset': [name],\n",
    "\t\t\t'avg_loss': [loss],\n",
    "\t\t\t'avg_accuracy': [scored],\n",
    "\t\t\t'total_time': [total_time]\n",
    "\t\t})], ignore_index=True)\n",
    "\n",
    "print('Finished all datasets')\n",
    "\n",
    "# print the result\n",
    "print(result_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d796772",
   "metadata": {},
   "source": [
    "4. print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              dataset     model  avg_loss  avg_accuracy  \\\n",
      "0           gi4e_full    ResNet  0.000011     99.981028   \n",
      "1           gi4e_full  DenseNet  0.000229     99.993595   \n",
      "2           gi4e_full       VGG  0.059837     99.096932   \n",
      "3       gi4e_raw_eyes    ResNet  0.004592     94.986418   \n",
      "4       gi4e_raw_eyes  DenseNet  0.005347     95.743190   \n",
      "5       gi4e_raw_eyes       VGG  0.119210     78.677075   \n",
      "6  gi4e_detected_eyes    ResNet  0.122154     94.245342   \n",
      "7  gi4e_detected_eyes  DenseNet  0.003370     97.697311   \n",
      "8  gi4e_detected_eyes       VGG  0.031279     93.412539   \n",
      "\n",
      "                 total_time  \n",
      "0 0 days 00:26:04.910116911  \n",
      "1 0 days 00:15:41.770978928  \n",
      "2 0 days 00:19:16.193440437  \n",
      "3 0 days 00:12:16.112033606  \n",
      "4 0 days 00:10:20.350670099  \n",
      "5 0 days 00:15:50.633701324  \n",
      "6 0 days 00:09:34.279892683  \n",
      "7 0 days 00:10:33.354367256  \n",
      "8 0 days 00:16:07.067535877  \n"
     ]
    }
   ],
   "source": [
    "# swap the first two columns\n",
    "result_df = result_df[['dataset', 'model', 'avg_loss', 'avg_accuracy', 'total_time']]\n",
    "# scale the avg_accuracy to 0-100\n",
    "result_df['avg_accuracy'] = 100 * (1 - result_df['avg_accuracy'])\n",
    "# display the total time in the format HH:MM:SS\n",
    "result_df['total_time'] = pd.to_timedelta(result_df['total_time'], unit='s')\n",
    "\n",
    "# save the result to csv\n",
    "result_df.to_csv(f'results_{time_str}.csv', index=False)\n",
    "# print the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              dataset     model  avg_loss  avg_accuracy  \\\n",
      "0           gi4e_full    ResNet  0.000011     99.981028   \n",
      "1           gi4e_full  DenseNet  0.000229     99.993595   \n",
      "2           gi4e_full       VGG  0.059837     99.096932   \n",
      "3       gi4e_raw_eyes    ResNet  0.004592     94.986418   \n",
      "4       gi4e_raw_eyes  DenseNet  0.005347     95.743190   \n",
      "5       gi4e_raw_eyes       VGG  0.119210     78.677075   \n",
      "6  gi4e_detected_eyes    ResNet  0.122154     94.245342   \n",
      "7  gi4e_detected_eyes  DenseNet  0.003370     97.697311   \n",
      "8  gi4e_detected_eyes       VGG  0.031279     93.412539   \n",
      "\n",
      "                 total_time  \n",
      "0 0 days 00:26:04.910116911  \n",
      "1 0 days 00:15:41.770978928  \n",
      "2 0 days 00:19:16.193440437  \n",
      "3 0 days 00:12:16.112033606  \n",
      "4 0 days 00:10:20.350670099  \n",
      "5 0 days 00:15:50.633701324  \n",
      "6 0 days 00:09:34.279892683  \n",
      "7 0 days 00:10:33.354367256  \n",
      "8 0 days 00:16:07.067535877  \n"
     ]
    }
   ],
   "source": [
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e46b4",
   "metadata": {},
   "source": [
    "# Embedded Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a6be8",
   "metadata": {},
   "source": [
    "- Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d460f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "embedded_models = [\n",
    "    md.embedded_resnet50(103),\n",
    "    md.embedded_densenet121(103),\n",
    "    md.embedded_vgg16(103)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa0cc06",
   "metadata": {},
   "source": [
    "- Train all defined model on each registered datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21efd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gi4e_full dataset with Classifier\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# do the train\u001b[39;00m\n\u001b[0;32m      7\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m scored, loss \u001b[38;5;241m=\u001b[39m \u001b[43mdoTheTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     10\u001b[0m total_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mdoTheTrain\u001b[1;34m(dataset, model)\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     12\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m86\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice: \u001b[39m\u001b[38;5;124m'\u001b[39m, trainer\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     16\u001b[0m avg_loss, metric \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcross_validate(train_ds, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32md:\\Workspace\\thesis_sources\\train\\trainer.py:43\u001b[0m, in \u001b[0;36mClassifierTrainer.__init__\u001b[1;34m(self, model, optimizer, loss_fn, random_seed_value, device)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random_seed_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m   \u001b[43mseed_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_seed_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Workspace\\thesis_sources\\train\\trainer.py:209\u001b[0m, in \u001b[0;36mseed_everything\u001b[1;34m(seed_value)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mseed_everything\u001b[39m(seed_value):\n\u001b[0;32m    208\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed_value)\n\u001b[1;32m--> 209\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    212\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed(seed_value)\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m disable_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\random.py:129\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m    126\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m    127\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m--> 129\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:249\u001b[0m, in \u001b[0;36m_lazy_call\u001b[1;34m(callable, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 249\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\random.py:127\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m    126\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m--> 127\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(columns=['model', 'avg_loss', 'avg_accuracy', 'total_time'])\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "\tfor model in embedded_models:\n",
    "\t\tprint(f'Running {name} dataset with {model.__class__.__name__}')\n",
    "\t\t# do the train\n",
    "\t\tstart_time = time.time()\n",
    "\t\tscored, loss = doTheTrain(dataset, model)\n",
    "\t\tend_time = time.time()\n",
    "\t\ttotal_time = end_time - start_time\n",
    "\t\tprint(f'Finished {name} dataset with {model.__class__.__name__}')\n",
    "\t\tprint('----------------------')\n",
    "\n",
    "\t\t# save the result\n",
    "\t\tresult_df = pd.concat([result_df, pd.DataFrame({\n",
    "\t\t\t'model': [model.__class__.__name__],\n",
    "\t\t\t'dataset': [name],\n",
    "\t\t\t'avg_loss': [loss],\n",
    "\t\t\t'avg_accuracy': [scored],\n",
    "\t\t\t'total_time': [total_time]\n",
    "\t\t})], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c5880",
   "metadata": {},
   "source": [
    "- Print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dbf0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap the first two columns\n",
    "result_df = result_df[['dataset', 'model', 'avg_loss', 'avg_accuracy', 'total_time']]\n",
    "# scale the avg_accuracy to 0-100\n",
    "result_df['avg_accuracy'] = 100 * (1 - result_df['avg_accuracy'])\n",
    "# display the total time in the format HH:MM:SS\n",
    "result_df['total_time'] = pd.to_timedelta(result_df['total_time'], unit='s')\n",
    "\n",
    "# save the result to csv\n",
    "result_df.to_csv(f'results_{time_str}.csv', index=False)\n",
    "# print the result\n",
    "print(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
