{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa27d18",
   "metadata": {},
   "source": [
    "# Cross-Validation Triplet Classification Training Notebook\n",
    "\n",
    "This notebook demonstrates various training processes for computer vision models, including:\n",
    "- Triplet loss training for feature extraction\n",
    "- Standard classification training  \n",
    "- Semantic classification with label embeddings\n",
    "- Active Learning with Reinforcement Learning (AL-RL)\n",
    "\n",
    "The implementation is based on the `cv_triplet_classification.py` script and provides an interactive way to explore different training methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2801eb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "PyTorch version: 2.5.1+cu124\n",
      "Device available: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import custom modules\n",
    "import dataset as ds\n",
    "from model import FeatureExtractor, Classifier, DDQNAgent\n",
    "\n",
    "# Define get_device function directly instead of importing from trainer\n",
    "def get_device():\n",
    "\t\"\"\"\n",
    "\tGet the device to use for PyTorch operations.\n",
    "\tReturns 'cuda' if GPU is available, 'cpu' otherwise.\n",
    "\t\"\"\"\n",
    "\treturn 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Global variables for tracking current model and dataset\n",
    "current_backbone_model = None\n",
    "current_dataset = None\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {get_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05f6a0",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "The following cells contain utility functions for model saving, validation, and semantic embedding computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_path, model):\n",
    "    \"\"\"\n",
    "    Save the model to the given path.\n",
    "    \n",
    "    Args:\n",
    "        model_path: The path to save the model to.\n",
    "        model: The model to save.\n",
    "    \"\"\"\n",
    "    # Ensure the model path is a string\n",
    "    if not isinstance(model_path, str):\n",
    "        raise ValueError(\"model_path must be a string\")\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(os.path.dirname(model_path)):\n",
    "        os.makedirs(os.path.dirname(model_path))\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "\n",
    "\n",
    "def validate_model(model, dataset, batch_size=64):\n",
    "    \"\"\"\n",
    "    Validate the model on the given dataset and return the average loss and accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to validate.\n",
    "        dataset: The dataset to validate on.\n",
    "        batch_size: The batch size to use for validation.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store true and predicted labels for metrics\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += predicted.tolist()\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Print the validation results\n",
    "    print(f'Validation average loss: {avg_loss}, Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}%, Recall: {recall:.2f}%, F1 Score: {f1:.2f}%')\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "print(\"Model utility functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_label_embeddings(labels, out_features):\n",
    "    \"\"\"\n",
    "    Compute label encodings for the given labels using BERT embeddings.\n",
    "    \n",
    "    Args:\n",
    "        labels: A list of labels to encode.\n",
    "        out_features: The desired output feature dimension.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary mapping labels to their semantic embeddings.\n",
    "    \"\"\"\n",
    "    labels = sorted(set(labels))  # Ensure unique labels\n",
    "    labels = [str(label) for label in labels]  # Convert labels to strings\n",
    "    \n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    nomic = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', \n",
    "                                     trust_remote_code=True, \n",
    "                                     safe_serialization=True)\n",
    "    \n",
    "    # Encode labels to embeddings\n",
    "    labels_embeddings = tokenizer(labels, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = nomic(**labels_embeddings)\n",
    "    \n",
    "    # Max pooling the label embeddings\n",
    "    token_embeddings = embeddings[0]\n",
    "    attention_mask = labels_embeddings['attention_mask']\n",
    "    \n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    # Scale the embeddings to out_features\n",
    "    embeddings = torch.nn.Linear(embeddings.shape[1], out_features)(embeddings)\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Store the embeddings with absolute values\n",
    "    labels_embeddings = abs(embeddings)\n",
    "    \n",
    "    # Create a dictionary to map labels to embeddings\n",
    "    label_to_embedding = {label: embedding for label, embedding in zip(labels, labels_embeddings)}\n",
    "    return label_to_embedding\n",
    "\n",
    "print(\"Semantic embedding function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest_semantic_embeddings(knowledge_dataset, target_embeddings, k=3, batch_size=64):\n",
    "    \"\"\"\n",
    "    Find the k nearest semantic embeddings for each embedding in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        knowledge_dataset: A dataset containing semantic embeddings to search in.\n",
    "        target_embeddings: The embeddings to find the nearest neighbors for.\n",
    "        k: The number of nearest neighbors to find.\n",
    "        batch_size: Batch size for processing.\n",
    "    \n",
    "    Returns:\n",
    "        A list of indices of the k nearest semantic embeddings.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    \n",
    "    # Collect knowledge dataset embeddings and labels\n",
    "    k_embeddings_list = []\n",
    "    k_labels_list = []\n",
    "    knowledge_dataloader = DataLoader(knowledge_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Extract embeddings from knowledge dataset\n",
    "    for k_embeddings, k_labels in knowledge_dataloader:\n",
    "        k_embeddings_list.append(k_embeddings.to(device))\n",
    "        k_labels_list.extend([str(label.item()) for label in k_labels])\n",
    "    \n",
    "    # Stack all knowledge embeddings into a single tensor for efficient computation\n",
    "    if not k_embeddings_list:\n",
    "        raise ValueError(\"Knowledge dataset is empty\")\n",
    "    k_embeddings_tensor = torch.cat(k_embeddings_list).to(device)\n",
    "    \n",
    "    # Compute pairwise distances efficiently (batch computation)\n",
    "    distances = torch.cdist(torch.stack(target_embeddings), k_embeddings_tensor)\n",
    "    # Get indices of k nearest neighbors for each target embedding\n",
    "    _, indices = torch.topk(distances, k, dim=1, largest=False)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "def semantic_validate_model(model, validate_dataset, knowledge_dataset, label_to_embeddings, batch_size=64):\n",
    "    \"\"\"\n",
    "    Validate the model on the given dataset using semantic embeddings and return the average loss and accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to validate (model must be Classifier).\n",
    "        validate_dataset: The dataset to validate on (already contains embeddings).\n",
    "        knowledge_dataset: The dataset containing knowledge embeddings (already contains embeddings).\n",
    "        label_to_embeddings: Dictionary mapping labels to their semantic embeddings.\n",
    "        batch_size: The batch size to use for validation.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss, accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    validate_dataloader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\n",
    "    knowledge_dataloader = DataLoader(knowledge_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_of_k_nearest = 3\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Collect knowledge dataset embeddings and labels\n",
    "        k_embeddings_list = []\n",
    "        k_labels_list = []\n",
    "        \n",
    "        # Extract embeddings from knowledge dataset\n",
    "        for k_embeddings, k_labels in knowledge_dataloader:\n",
    "            k_embeddings_list.append(k_embeddings.to(device))\n",
    "            k_labels_list.extend([str(label.item()) for label in k_labels])\n",
    "        \n",
    "        # Stack all knowledge embeddings into a single tensor for efficient computation\n",
    "        if not k_embeddings_list:\n",
    "            raise ValueError(\"Knowledge dataset is empty\")\n",
    "        \n",
    "        k_embeddings_tensor = torch.cat(k_embeddings_list).to(device)\n",
    "        \n",
    "        # Now validate on the validation dataset\n",
    "        for val_embeddings, labels in validate_dataloader:\n",
    "            val_embeddings, labels = val_embeddings.to(device), labels.to(device)\n",
    "            # Compute pairwise distances efficiently (batch computation)\n",
    "            distances = torch.cdist(val_embeddings, k_embeddings_tensor)\n",
    "            \n",
    "            # Get indices of k nearest neighbors for each validation embedding\n",
    "            _, indices = torch.topk(distances, num_of_k_nearest, dim=1, largest=False)\n",
    "            \n",
    "            # Prepare label embeddings tensor once\n",
    "            label_embeddings_tensor = torch.stack([label_to_embeddings[label].to(device) for label in k_labels_list])\n",
    "            \n",
    "            # Gather the embeddings of the nearest neighbors\n",
    "            batch_size_val, k = indices.size()\n",
    "            nearest_embeddings = label_embeddings_tensor[indices.view(-1)].view(batch_size_val, k, -1)\n",
    "            \n",
    "            # Calculate centroids for each validation point\n",
    "            centroid_embeddings = torch.mean(nearest_embeddings, dim=1)\n",
    "            \n",
    "            # Subtract the centroid embeddings from the validation embeddings\n",
    "            val_embeddings -= centroid_embeddings\n",
    "            \n",
    "            # Use the transformed embeddings for final classification\n",
    "            outputs = model(val_embeddings)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Store true and predicted labels for metrics\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += predicted.tolist()\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(validate_dataloader)\n",
    "    \n",
    "    print(f'Semantic Validation average loss: {avg_loss:.4f}, '\n",
    "          f'Accuracy: {100 * accuracy:.2f}%, '\n",
    "          f'Precision: {100 * precision:.2f}%, '\n",
    "          f'Recall: {100 * recall:.2f}%, '\n",
    "          f'F1 Score: {100 * f1:.2f}%')\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "print(\"Semantic validation and k-nearest neighbor functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f52f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_most_informative_samples_indices(dataset, model, samples_taken_indices, n=2, batch_size=64):\n",
    "    \"\"\"\n",
    "    Get the n most informative samples from the dataset using the model's uncertainty.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to get samples from.\n",
    "        model: The model to use for uncertainty estimation.\n",
    "        samples_taken_indices: List of indices already taken (to exclude).\n",
    "        n: The number of samples to return.\n",
    "        batch_size: Batch size for processing.\n",
    "    \n",
    "    Returns:\n",
    "        List of indices of the n most informative samples.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    \n",
    "    # Create a data loader for the dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    outputs = []\n",
    "    # Get the outputs for all samples in the dataset\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[0].to(device)\n",
    "            outputs.append(model(inputs).cpu())\n",
    "    outputs = torch.cat(outputs)\n",
    "    \n",
    "    # Calculate entropy to find the most informative samples\n",
    "    with torch.no_grad():\n",
    "        # Apply softmax to get probability distributions\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Calculate entropy for each sample: -sum(p * log(p))\n",
    "        # Handle zero probabilities by adding a small epsilon to avoid log(0)\n",
    "        epsilon = 1e-10\n",
    "        entropy = -torch.sum(probs * torch.log(probs + epsilon), dim=1)\n",
    "        \n",
    "        # Sort samples by entropy (higher entropy = more uncertainty = more informative)\n",
    "        entropy_np = entropy.cpu().numpy()\n",
    "        # Exclude already taken samples\n",
    "        entropy_np[samples_taken_indices] = -np.inf  # Set taken samples' entropy to -inf to exclude them\n",
    "        most_uncertain_indices = np.argsort(entropy_np)[::-1][:n]\n",
    "    \n",
    "    # Return the indices of the n most informative samples\n",
    "    return most_uncertain_indices.tolist()\n",
    "\n",
    "print(\"Active learning utility function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed643e9e",
   "metadata": {},
   "source": [
    "## Training Processes\n",
    "\n",
    "This section implements different training methodologies:\n",
    "1. **Triplet Training**: Uses triplet loss to learn embeddings where similar images are close and dissimilar images are far apart\n",
    "2. **Classification Training**: Standard supervised learning for classification\n",
    "3. **Semantic Classification**: Classification with semantic label embeddings\n",
    "4. **Active Learning + Reinforcement Learning**: Advanced training with intelligent sample selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d70984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_train_process(dataset, model, k_fold=5, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train the model using triplet loss with k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The triplet dataset for training\n",
    "        model: The feature extractor model\n",
    "        k_fold: Number of folds for cross-validation\n",
    "        batch_size: Batch size for training\n",
    "    \n",
    "    Returns:\n",
    "        Trained model, average loss, average test loss\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    \n",
    "    # Split dataset using k-fold cross-validation\n",
    "    dataset_size = len(dataset)\n",
    "    fold_size = dataset_size // k_fold\n",
    "    \n",
    "    for fold in range(k_fold):\n",
    "        print(f'Running fold {fold + 1}/{k_fold}...')\n",
    "        \n",
    "        # Initialize the trainer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = torch.nn.TripletMarginLoss(margin=0.2, p=2)\n",
    "        \n",
    "        # Split dataset into train and validation sets\n",
    "        train_dataset = torch.utils.data.Subset(dataset, list(\n",
    "            range(fold_size * fold)) + list(range(fold_size * (fold + 1), dataset_size)))\n",
    "        val_dataset = torch.utils.data.Subset(dataset, range(fold_size * fold, fold_size * (fold + 1)))\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Train the model\n",
    "        epochs = 10\n",
    "        loss_list = []\n",
    "        test_loss_list = []\n",
    "        \n",
    "        # Loop through epochs\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch + 1}/{epochs}...')\n",
    "            start_time = time.time()\n",
    "            model = model.to(device)\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            # Training loop\n",
    "            for batch in train_loader:\n",
    "                anchor, positive, negative = batch\n",
    "                anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output_anchor = model(anchor)\n",
    "                output_positive = model(positive)\n",
    "                output_negative = model(negative)\n",
    "                \n",
    "                loss = loss_fn(output_anchor, output_positive, output_negative)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            total_test_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    anchor, positive, negative = batch\n",
    "                    anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "                    \n",
    "                    output_anchor = model(anchor)\n",
    "                    output_positive = model(positive)\n",
    "                    output_negative = model(negative)\n",
    "                    \n",
    "                    loss = loss_fn(output_anchor, output_positive, output_negative)\n",
    "                    total_test_loss += loss.item()\n",
    "            \n",
    "            # Calculate average loss for the epoch\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            avg_test_loss = total_test_loss / len(val_loader)\n",
    "            print(f'Fold {fold + 1}: Average loss: {avg_loss:.4f}, Average test loss: {avg_test_loss:.4f}')\n",
    "            print(f'Time taken for fold {fold + 1}, epoch {epoch + 1}: {time.time() - start_time:.2f} seconds')\n",
    "            loss_list.append(avg_loss)\n",
    "            test_loss_list.append(avg_test_loss)\n",
    "        \n",
    "        print(f'Fold {fold + 1} completed.')\n",
    "        \n",
    "        # Save the model after each fold\n",
    "        model_dir = f'models/triplet/{current_dataset}_{model._get_name()}'\n",
    "        save_model(f'{model_dir}/model_fold_{fold + 1}.pth', model)\n",
    "    \n",
    "    # Print the average loss over all folds\n",
    "    average_loss = sum(loss_list) / len(loss_list)\n",
    "    average_test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "    print(f'Over all folds: Average loss: {average_loss:.4f}, Average test loss: {average_test_loss:.4f}')\n",
    "    \n",
    "    return model, average_loss, average_test_loss\n",
    "\n",
    "print(\"Triplet training process function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800580f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_train_process(dataset, model, k_fold=5, batch_size=64, test_dataset=None):\n",
    "    \"\"\"\n",
    "    Train the model using standard classification with k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The classification dataset for training\n",
    "        model: The classifier model\n",
    "        k_fold: Number of folds for cross-validation\n",
    "        batch_size: Batch size for training\n",
    "        test_dataset: Optional test dataset for evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Trained model, average loss, average validation loss\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    \n",
    "    # Create result dataframe\n",
    "    result_df = pd.DataFrame(columns=['dataset', 'model', 'fold', 'avg_loss', 'avg_test_loss',\n",
    "                                    'avg_val_loss', 'precision', 'recall', 'f1', 'accuracy'])\n",
    "    \n",
    "    for fold in range(k_fold):\n",
    "        print(f'Running fold {fold + 1}/{k_fold}...')\n",
    "        fold_start_time = time.time()\n",
    "        \n",
    "        # Initialize the trainer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Split dataset into train and validation sets\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Train the model\n",
    "        epochs = 10\n",
    "        loss_list = []\n",
    "        val_loss_list = []\n",
    "        \n",
    "        # Loop through epochs\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch + 1}/{epochs}...')\n",
    "            epoch_start_time = time.time()\n",
    "            model = model.to(device)\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            # Training loop\n",
    "            for batch in train_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            \n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            total_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    inputs, labels = batch\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    total_val_loss += loss.item()\n",
    "            \n",
    "            # Calculate average loss for the epoch\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            print(f'Fold {fold + 1}: Average loss: {avg_loss:.4f}, Average validation loss: {avg_val_loss:.4f}')\n",
    "            print(f'Time taken for fold {fold + 1}, epoch {epoch + 1}: {epoch_end_time - epoch_start_time:.2f} seconds')\n",
    "            loss_list.append(avg_loss)\n",
    "            val_loss_list.append(avg_val_loss)\n",
    "        \n",
    "        print(f'Fold {fold + 1} completed.')\n",
    "        \n",
    "        # Save the model after each fold\n",
    "        model_dir = f'models/classification/{current_dataset}_{model._get_name()}'\n",
    "        save_model(f'{model_dir}/model_fold_{fold + 1}.pth', model)\n",
    "        \n",
    "        # Validate the model on the test set if provided\n",
    "        if test_dataset is not None:\n",
    "            avg_test_loss, accuracy, precision, recall, f1 = validate_model(model, test_dataset, batch_size=batch_size)\n",
    "            print(f'Fold {fold + 1}: Test loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "            \n",
    "            fold_end_time = time.time()\n",
    "            result_df = pd.concat([result_df, pd.DataFrame({\n",
    "                'dataset': [dataset.__class__.__name__],\n",
    "                'model': [model._get_name()],\n",
    "                'fold': [fold + 1],\n",
    "                'avg_loss': [avg_loss],\n",
    "                'avg_test_loss': [avg_test_loss],\n",
    "                'avg_val_loss': [avg_val_loss],\n",
    "                'accuracy': [accuracy],\n",
    "                'precision': [precision],\n",
    "                'recall': [recall],\n",
    "                'f1': [f1],\n",
    "                'total_time': [fold_end_time - fold_start_time]\n",
    "            })], ignore_index=True)\n",
    "    \n",
    "    # Print the average loss over all folds\n",
    "    average_loss = sum(loss_list) / len(loss_list)\n",
    "    average_val_loss = sum(val_loss_list) / len(val_loss_list)\n",
    "    print(f'Over all folds: Average loss: {average_loss:.4f}, Average validation loss: {average_val_loss:.4f}')\n",
    "    \n",
    "    return model, average_loss, average_val_loss\n",
    "\n",
    "print(\"Classification training process function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_classification_train_process(dataset, model, semantic_embedding_fn, k_fold=5, batch_size=64, test_dataset=None):\n",
    "    \"\"\"\n",
    "    Train the model using semantic classification with label embeddings and k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The classification dataset for training (must have labels attribute)\n",
    "        model: The classifier model (must have backbone_out_features attribute)\n",
    "        semantic_embedding_fn: Function to compute semantic embeddings\n",
    "        k_fold: Number of folds for cross-validation\n",
    "        batch_size: Batch size for training\n",
    "        test_dataset: Optional test dataset for evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Trained model, average loss, average validation loss, and label embeddings\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    \n",
    "    # Create result dataframe\n",
    "    result_df = pd.DataFrame(columns=['dataset', 'model', 'fold', 'avg_loss', 'avg_test_loss',\n",
    "                                    'avg_val_loss', 'precision', 'recall', 'f1', 'accuracy'])\n",
    "    \n",
    "    # Compute label embeddings\n",
    "    print('Computing label embeddings...')\n",
    "    label_to_embedding = compute_label_embeddings(dataset.labels, model.backbone_out_features)\n",
    "    \n",
    "    if not semantic_embedding_fn:\n",
    "        # Default to zero if no function is provided, it means no semantic embeddings are used\n",
    "        def semantic_embedding_fn(x): return 0\n",
    "    \n",
    "    label_to_embedding = {label: semantic_embedding_fn(index) * embedding \n",
    "                         for index, (label, embedding) in enumerate(label_to_embedding.items())}\n",
    "    print('Label embeddings computed.')\n",
    "    \n",
    "    # Split dataset using k-fold cross-validation\n",
    "    dataset_size = len(dataset)\n",
    "    fold_size = dataset_size // k_fold\n",
    "    \n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    for fold in range(k_fold):\n",
    "        print(f'Running fold {fold + 1}/{k_fold}...')\n",
    "        fold_start_time = time.time()\n",
    "        \n",
    "        # Initialize the trainer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Split dataset into train and validation sets\n",
    "        train_dataset = torch.utils.data.Subset(dataset, list(\n",
    "            range(fold_size * fold)) + list(range(fold_size * (fold + 1), dataset_size)))\n",
    "        val_dataset = torch.utils.data.Subset(dataset, range(fold_size * fold, fold_size * (fold + 1)))\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Train the model\n",
    "        epochs = 10\n",
    "        \n",
    "        # Loop through epochs\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch + 1}/{epochs}...')\n",
    "            epoch_start_time = time.time()\n",
    "            model = model.to(device)\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            # Training loop with semantic embeddings\n",
    "            for batch in train_loader:\n",
    "                inputs, labels = batch\n",
    "                \n",
    "                # Move input embeddings to semantic embeddings\n",
    "                label_embeddings = torch.stack([label_to_embedding[str(label.item())].detach() for label in labels])\n",
    "                # Subtract the label embeddings from the inputs\n",
    "                inputs = inputs - label_embeddings.to(device)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            \n",
    "            # Validation using semantic validation\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            avg_val_loss, val_accuracy, val_precision, val_recall, val_f1 = semantic_validate_model(\n",
    "                model, val_dataset, train_dataset, label_to_embedding, batch_size=batch_size)\n",
    "            \n",
    "            print(f'Fold {fold + 1}, Epoch {epoch + 1}: Average loss: {avg_loss:.4f}, '\n",
    "                  f'Validation loss: {avg_val_loss:.4f}, Validation accuracy: {val_accuracy:.2f}%, '\n",
    "                  f'Time taken: {epoch_end_time - epoch_start_time:.2f} seconds')\n",
    "            \n",
    "            loss_list.append(avg_loss)\n",
    "            val_loss_list.append(avg_val_loss)\n",
    "        \n",
    "        fold_end_time = time.time()\n",
    "        print(f'Fold {fold + 1} completed.')\n",
    "        \n",
    "        # Save the model after each fold\n",
    "        model_dir = f'models/classification/{current_dataset}_{model._get_name()}'\n",
    "        save_model(f'{model_dir}/model_fold_{fold + 1}.pth', model)\n",
    "        \n",
    "        # Validate the model on the test set if provided\n",
    "        if test_dataset is not None:\n",
    "            avg_test_loss, accuracy, precision, recall, f1 = semantic_validate_model(\n",
    "                model, test_dataset, train_dataset, label_to_embedding, batch_size=batch_size)\n",
    "            print(f'Fold {fold + 1}: Average test loss: {avg_test_loss:.4f}, '\n",
    "                  f'Test accuracy: {accuracy:.2f}%, Test precision: {precision:.2f}%, '\n",
    "                  f'Test recall: {recall:.2f}%, Test F1 Score: {f1:.2f}%')\n",
    "            \n",
    "            result_df = pd.concat([result_df, pd.DataFrame({\n",
    "                'dataset': current_dataset,\n",
    "                'model': model._get_name(),\n",
    "                'fold': fold + 1,\n",
    "                'avg_loss': total_loss / len(train_loader),\n",
    "                'avg_test_loss': avg_test_loss,\n",
    "                'avg_val_loss': avg_val_loss,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'accuracy': accuracy,\n",
    "                'total_time': fold_end_time - fold_start_time\n",
    "            }, index=[0])], ignore_index=True)\n",
    "    \n",
    "    # Print the average loss over all folds\n",
    "    average_loss = sum(loss_list) / len(loss_list)\n",
    "    average_val_loss = sum(val_loss_list) / len(val_loss_list)\n",
    "    print(f'Over all folds: Average loss: {average_loss:.4f}, Average validation loss: {average_val_loss:.4f}')\n",
    "    \n",
    "    # Save results to CSV\n",
    "    if not result_df.empty:\n",
    "        result_dir = 'results/cv-triplet'\n",
    "        if not os.path.exists(result_dir):\n",
    "            os.makedirs(result_dir)\n",
    "        result_path = f'{result_dir}/{current_dataset}_{current_backbone_model._get_name()}_{time.strftime(\"%Y%m%d-%H%M%S\")}.csv'\n",
    "        result_df.to_csv(result_path, index=False)\n",
    "        print(f'Results saved to {result_path}')\n",
    "    \n",
    "    return model, average_loss, average_val_loss, label_to_embedding\n",
    "\n",
    "print(\"Semantic classification training process function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21791293",
   "metadata": {},
   "source": [
    "## Active Learning with Semantic Classification\n",
    "\n",
    "This section demonstrates how to use active learning techniques to intelligently select the most informative samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_demo(model, train_dataset, val_dataset, label_embeddings, num_iterations=3, samples_per_iteration=5):\n",
    "    \"\"\"\n",
    "    Demonstrate active learning by iteratively selecting the most informative samples.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model to use for uncertainty estimation\n",
    "        train_dataset: Training dataset \n",
    "        val_dataset: Validation dataset to select samples from\n",
    "        label_embeddings: Dictionary of label embeddings\n",
    "        num_iterations: Number of active learning iterations\n",
    "        samples_per_iteration: Number of samples to select per iteration\n",
    "    \n",
    "    Returns:\n",
    "        List of selected sample indices and their uncertainties\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    model.eval()\n",
    "    \n",
    "    samples_taken_indices = []\n",
    "    iteration_results = []\n",
    "    \n",
    "    print(\"=== Active Learning Demonstration ===\")\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\n--- Iteration {iteration + 1}/{num_iterations} ---\")\n",
    "        \n",
    "        # Get the most informative samples\n",
    "        informative_indices = get_n_most_informative_samples_indices(\n",
    "            val_dataset, model, samples_taken_indices, n=samples_per_iteration\n",
    "        )\n",
    "        \n",
    "        print(f\"Selected sample indices: {informative_indices}\")\n",
    "        \n",
    "        # Add to taken samples\n",
    "        samples_taken_indices.extend(informative_indices)\n",
    "        \n",
    "        # Get the actual samples and their features\n",
    "        informative_samples = [val_dataset[i] for i in informative_indices]\n",
    "        \n",
    "        # Find k-nearest neighbors for each selected sample\n",
    "        sample_embeddings = [sample[0] for sample in informative_samples]\n",
    "        \n",
    "        try:\n",
    "            k_nearest_indices = find_k_nearest_semantic_embeddings(\n",
    "                train_dataset, sample_embeddings, k=3\n",
    "            )\n",
    "            print(f\"Found k-nearest neighbors for selected samples\")\n",
    "            \n",
    "            # Calculate semantic adjustments\n",
    "            semantic_adjustments = []\n",
    "            for i, sample_idx in enumerate(informative_indices):\n",
    "                sample_embedding = sample_embeddings[i]\n",
    "                nearest_indices = k_nearest_indices[i]\n",
    "                \n",
    "                # Get semantic embeddings of nearest neighbors\n",
    "                nearest_labels = [str(train_dataset[idx][1]) for idx in nearest_indices]\n",
    "                nearest_semantic_embeddings = [label_embeddings[label] for label in nearest_labels]\n",
    "                \n",
    "                # Calculate centroid\n",
    "                centroid = torch.mean(torch.stack(nearest_semantic_embeddings), dim=0)\n",
    "                semantic_adjustments.append(centroid)\n",
    "                \n",
    "                print(f\"  Sample {sample_idx}: Nearest labels {nearest_labels}\")\n",
    "            \n",
    "            iteration_results.append({\n",
    "                'iteration': iteration + 1,\n",
    "                'selected_indices': informative_indices,\n",
    "                'semantic_adjustments': semantic_adjustments\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in k-nearest neighbor search: {e}\")\n",
    "            iteration_results.append({\n",
    "                'iteration': iteration + 1,\n",
    "                'selected_indices': informative_indices,\n",
    "                'semantic_adjustments': None\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nActive Learning completed. Total samples selected: {len(samples_taken_indices)}\")\n",
    "    return samples_taken_indices, iteration_results\n",
    "\n",
    "\n",
    "# Example usage (commented out to avoid execution)\n",
    "# # Assuming you have a trained model and datasets:\n",
    "# # selected_samples, al_results = active_learning_demo(\n",
    "# #     trained_model, embedded_train_ds, embedded_test_ds, label_embeddings, \n",
    "# #     num_iterations=3, samples_per_iteration=2\n",
    "# # )\n",
    "\n",
    "print(\"Active Learning demonstration function defined!\")\n",
    "print(\"This function shows how to:\")\n",
    "print(\"1. Select the most uncertain/informative samples\")\n",
    "print(\"2. Find their k-nearest neighbors in the training set\")\n",
    "print(\"3. Calculate semantic adjustments based on neighbor embeddings\")\n",
    "print(\"4. Iteratively improve the model with selected samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a67c5",
   "metadata": {},
   "source": [
    "## Dataset Creation and Configuration\n",
    "\n",
    "This section defines functions to create different types of datasets for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd231e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_process_df(dataset_type, create_triplet_dataset_fn, create_classification_dataset_fn, \n",
    "                               create_classification_test_dataset_fn=None, models=['resnet'], batch_size=32):\n",
    "    \"\"\"\n",
    "    Create a DataFrame to store the training process information.\n",
    "    \n",
    "    Args:\n",
    "        dataset_type: The type of dataset to use (e.g., 'cifar10', 'mnist').\n",
    "        create_triplet_dataset_fn: Function to create the triplet dataset.\n",
    "        create_classification_dataset_fn: Function to create the classification dataset.\n",
    "        create_classification_test_dataset_fn: Function to create the classification test dataset (optional).\n",
    "        models: List of model names to include in the DataFrame.\n",
    "        batch_size: The batch size to use for training.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the training process information.\n",
    "    \"\"\"\n",
    "    triplet_df = pd.DataFrame(columns=[\n",
    "        'backbone_model', 'feature_extractor_model', 'dataset_type',\n",
    "        'create_triplet_dataset_fn', 'create_classification_dataset_fn',\n",
    "        'create_classification_test_dataset_fn', 'batch_size'\n",
    "    ])\n",
    "    \n",
    "    # Create backbone model functions\n",
    "    create_backbone_model_funcs = []\n",
    "    for model in models:\n",
    "        if model == 'resnet':\n",
    "            create_backbone_model_funcs.append(lambda: torchvision.models.resnet50(weights=None))\n",
    "        elif model == 'vgg':\n",
    "            create_backbone_model_funcs.append(lambda: torchvision.models.vgg16(weights=None))\n",
    "        elif model == 'mobilenet':\n",
    "            create_backbone_model_funcs.append(lambda: torchvision.models.mobilenet_v2(weights=None))\n",
    "        elif model == 'densenet':\n",
    "            create_backbone_model_funcs.append(lambda: torchvision.models.densenet121(weights=None))\n",
    "        else:\n",
    "            raise ValueError(f'Unknown model type: {model}')\n",
    "    \n",
    "    # Add all backbone models to the DataFrame\n",
    "    for create_fn in create_backbone_model_funcs:\n",
    "        triplet_df = pd.concat([triplet_df, pd.DataFrame({\n",
    "            'backbone_model': [create_fn()],\n",
    "            'feature_extractor_model': [FeatureExtractor(create_fn())],\n",
    "            'dataset_type': [dataset_type],\n",
    "            'create_triplet_dataset_fn': [create_triplet_dataset_fn],\n",
    "            'create_classification_dataset_fn': [create_classification_dataset_fn],\n",
    "            'create_classification_test_dataset_fn': [create_classification_test_dataset_fn],\n",
    "            'batch_size': [batch_size]\n",
    "        })], ignore_index=True)\n",
    "    \n",
    "    return triplet_df\n",
    "\n",
    "def create_train_test_dataset(create_train_dataset_fn, create_test_dataset_fn=None):\n",
    "    \"\"\"\n",
    "    Create a train and test dataset from the given functions.\n",
    "    \"\"\"\n",
    "    # Create the train dataset\n",
    "    train_dataset = create_train_dataset_fn()\n",
    "    \n",
    "    # If a test dataset function is provided, use it to create the test dataset\n",
    "    if create_test_dataset_fn is not None:\n",
    "        test_dataset = create_test_dataset_fn()\n",
    "    else:\n",
    "        # Otherwise, split the train dataset into train and test sets\n",
    "        train_size = int(0.8 * len(train_dataset))\n",
    "        test_size = len(train_dataset) - train_size\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "print(\"Dataset creation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b71983",
   "metadata": {},
   "source": [
    "## Example Dataset Configurations\n",
    "\n",
    "The following cells demonstrate how to configure different datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87668df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: gi4e dataset configuration\n",
    "def create_gi4e_triplet_dataset():\n",
    "    \"\"\"Create gi4e triplet dataset\"\"\"\n",
    "    return ds.TripletGi4eDataset(\n",
    "        './datasets/gi4e',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "def create_gi4e_classification_dataset():\n",
    "    \"\"\"Create gi4e classification dataset\"\"\"\n",
    "    return ds.Gi4eDataset(\n",
    "        './datasets/gi4e',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ]),\n",
    "        is_classification=True\n",
    "    )\n",
    "\n",
    "# Example: Raw eyes dataset configuration\n",
    "def create_gi4e_raw_eyes_triplet_dataset():\n",
    "    \"\"\"Create gi4e raw eyes triplet dataset\"\"\"\n",
    "    return ds.TripletImageDataset(\n",
    "        './datasets/gi4e_raw_eyes',\n",
    "        file_extension='png',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "def create_gi4e_raw_eyes_classification_dataset():\n",
    "    \"\"\"Create gi4e raw eyes classification dataset\"\"\"\n",
    "    return ds.ImageDataset(\n",
    "        './datasets/gi4e_raw_eyes',\n",
    "        file_extension='png',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "# Example: CelebA dataset configuration\n",
    "def create_celeba_triplet_dataset():\n",
    "    \"\"\"Create CelebA triplet dataset\"\"\"\n",
    "    return ds.TripletImageDataset(\n",
    "        './datasets/CelebA_HQ_facial_identity_dataset/train',\n",
    "        file_extension='jpg',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "def create_celeba_classification_dataset():\n",
    "    \"\"\"Create CelebA classification dataset\"\"\"\n",
    "    return ds.ImageDataset(\n",
    "        './datasets/CelebA_HQ_facial_identity_dataset/train',\n",
    "        file_extension='jpg',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "def create_celeba_test_dataset():\n",
    "    \"\"\"Create CelebA test dataset\"\"\"\n",
    "    return ds.ImageDataset(\n",
    "        './datasets/CelebA_HQ_facial_identity_dataset/test',\n",
    "        file_extension='jpg',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "print(\"Example dataset configurations defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e313a",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "\n",
    "This section demonstrates how to execute the training process for different models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f929c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, train_process='triplet', semantic_embedding_fn=None, k_fold=5, batch_size=32, test_dataset=None):\n",
    "    \"\"\"\n",
    "    Main training function that handles different training processes.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to train on.\n",
    "        model: The model to train.\n",
    "        train_process: The training process to use ('triplet', 'classification', 'semantic_classification').\n",
    "        semantic_embedding_fn: Function to compute semantic embeddings (optional).\n",
    "        k_fold: The number of folds for k-fold cross-validation.\n",
    "        batch_size: The batch size to use for training.\n",
    "        test_dataset: Optional test dataset for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        Trained model, average loss, average test loss, and label embeddings (if applicable).\n",
    "    \"\"\"\n",
    "    print('Starting training process...')\n",
    "    \n",
    "    if train_process == 'triplet':\n",
    "        trained_model, avg_loss, avg_test_loss = triplet_train_process(\n",
    "            dataset, model, k_fold=k_fold, batch_size=batch_size)\n",
    "        label_to_embedding = None\n",
    "        \n",
    "    elif train_process == 'classification':\n",
    "        trained_model, avg_loss, avg_test_loss = classification_train_process(\n",
    "            dataset, model, k_fold=k_fold, batch_size=batch_size, test_dataset=test_dataset)\n",
    "        label_to_embedding = None\n",
    "        \n",
    "    elif train_process == 'semantic_classification':\n",
    "        # Use the complete semantic classification training process\n",
    "        trained_model, avg_loss, avg_test_loss, label_to_embedding = semantic_classification_train_process(\n",
    "            dataset, model, semantic_embedding_fn=semantic_embedding_fn, \n",
    "            k_fold=k_fold, batch_size=batch_size, test_dataset=test_dataset)\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(f'Unknown training process: {train_process}')\n",
    "    \n",
    "    print('Training completed.')\n",
    "    return trained_model, avg_loss, avg_test_loss, label_to_embedding\n",
    "\n",
    "print(\"Updated main training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd40a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Example: Training with Multiple Approaches\n",
    "\n",
    "# Set global variables\n",
    "current_dataset = \"gi4e_raw_eyes_demo\"\n",
    "batch_size = 32\n",
    "\n",
    "# Create model\n",
    "backbone_model = torchvision.models.resnet50(weights=None)\n",
    "current_backbone_model = backbone_model\n",
    "feature_extractor = FeatureExtractor(backbone_model)\n",
    "\n",
    "print(f\"Created ResNet50 feature extractor with backbone: {backbone_model.__class__.__name__}\")\n",
    "print(f\"Feature extractor name: {feature_extractor._get_name()}\")\n",
    "\n",
    "# Semantic embedding functions for demonstration\n",
    "def linear_semantic_fn(x):\n",
    "    \"\"\"Linear scaling of embeddings\"\"\"\n",
    "    return x\n",
    "\n",
    "def quadratic_semantic_fn(x):\n",
    "    \"\"\"Quadratic scaling of embeddings\"\"\"\n",
    "    return 4 * x\n",
    "\n",
    "print(\"Semantic embedding functions defined.\")\n",
    "\n",
    "# Note: Uncomment the following sections to actually run training\n",
    "# This is commented out to avoid long execution times in the notebook\n",
    "\n",
    "print(\"\\n=== Training Workflow Demo ===\")\n",
    "print(\"1. First, train a triplet model for feature extraction\")\n",
    "print(\"2. Then use the trained features for classification\")\n",
    "print(\"3. Finally, apply semantic classification with label embeddings\")\n",
    "\n",
    "# # 1. Create datasets\n",
    "# print(\"\\n--- Step 1: Creating datasets ---\")\n",
    "# try:\n",
    "#     triplet_dataset = create_gi4e_raw_eyes_triplet_dataset()\n",
    "#     classification_dataset = create_gi4e_raw_eyes_classification_dataset()\n",
    "#     print(f\"Triplet dataset size: {len(triplet_dataset)}\")\n",
    "#     print(f\"Classification dataset size: {len(classification_dataset)}\")\n",
    "#     \n",
    "#     # Split into train and test\n",
    "#     train_ds, test_ds = create_train_test_dataset(create_gi4e_raw_eyes_classification_dataset)\n",
    "#     print(f\"Train dataset size: {len(train_ds)}\")\n",
    "#     print(f\"Test dataset size: {len(test_ds)}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error creating datasets: {e}\")\n",
    "#     print(\"Please ensure the dataset paths exist\")\n",
    "\n",
    "# # 2. Train triplet model for feature extraction\n",
    "# print(\"\\n--- Step 2: Training triplet model ---\")\n",
    "# trained_triplet, avg_loss_triplet, avg_test_loss_triplet, _ = train(\n",
    "#     triplet_dataset, feature_extractor, train_process='triplet',\n",
    "#     k_fold=2, batch_size=batch_size  # Reduced folds for demo\n",
    "# )\n",
    "# print(f\"Triplet training completed. Average loss: {avg_loss_triplet:.4f}\")\n",
    "\n",
    "# # 3. Create embedded datasets using trained triplet model\n",
    "# print(\"\\n--- Step 3: Creating embedded datasets ---\")\n",
    "# embedded_train_ds = ds.EmbeddedDataset(train_ds, trained_triplet, is_moving_labels_to_function=False)\n",
    "# embedded_test_ds = ds.EmbeddedDataset(test_ds, trained_triplet, is_moving_labels_to_function=False)\n",
    "# print(f\"Embedded train dataset size: {len(embedded_train_ds)}\")\n",
    "# print(f\"Embedded test dataset size: {len(embedded_test_ds)}\")\n",
    "\n",
    "# # 4. Train standard classifier on embedded features\n",
    "# print(\"\\n--- Step 4: Training standard classifier ---\")\n",
    "# classifier = Classifier(trained_triplet)\n",
    "# trained_classifier, avg_loss_class, avg_test_loss_class, _ = train(\n",
    "#     embedded_train_ds, classifier, train_process='classification',\n",
    "#     k_fold=2, batch_size=batch_size, test_dataset=embedded_test_ds\n",
    "# )\n",
    "# print(f\"Classification training completed. Average loss: {avg_loss_class:.4f}\")\n",
    "\n",
    "# # 5. Train semantic classifier with linear scaling\n",
    "# print(\"\\n--- Step 5: Training semantic classifier (linear scaling) ---\")\n",
    "# semantic_classifier_linear = Classifier(trained_triplet)\n",
    "# trained_semantic_linear, avg_loss_sem_lin, avg_test_loss_sem_lin, label_embeddings_lin = train(\n",
    "#     embedded_train_ds, semantic_classifier_linear, train_process='semantic_classification',\n",
    "#     semantic_embedding_fn=linear_semantic_fn, k_fold=2, batch_size=batch_size, test_dataset=embedded_test_ds\n",
    "# )\n",
    "# print(f\"Semantic classification (linear) completed. Average loss: {avg_loss_sem_lin:.4f}\")\n",
    "\n",
    "# # 6. Train semantic classifier with quadratic scaling\n",
    "# print(\"\\n--- Step 6: Training semantic classifier (quadratic scaling) ---\")\n",
    "# semantic_classifier_quad = Classifier(trained_triplet)\n",
    "# trained_semantic_quad, avg_loss_sem_quad, avg_test_loss_sem_quad, label_embeddings_quad = train(\n",
    "#     embedded_train_ds, semantic_classifier_quad, train_process='semantic_classification',\n",
    "#     semantic_embedding_fn=quadratic_semantic_fn, k_fold=2, batch_size=batch_size, test_dataset=embedded_test_ds\n",
    "# )\n",
    "# print(f\"Semantic classification (quadratic) completed. Average loss: {avg_loss_sem_quad:.4f}\")\n",
    "\n",
    "# # 7. Compare results\n",
    "# print(\"\\n--- Step 7: Results Comparison ---\")\n",
    "# results_comparison = pd.DataFrame({\n",
    "#     'Method': ['Triplet Only', 'Standard Classification', 'Semantic (Linear)', 'Semantic (Quadratic)'],\n",
    "#     'Average Loss': [avg_loss_triplet, avg_loss_class, avg_loss_sem_lin, avg_loss_sem_quad],\n",
    "#     'Test Loss': [avg_test_loss_triplet, avg_test_loss_class, avg_test_loss_sem_lin, avg_test_loss_sem_quad]\n",
    "# })\n",
    "# print(results_comparison)\n",
    "\n",
    "print(\"\\nTraining workflow example setup complete!\")\n",
    "print(\"Uncomment the code above to run the complete training pipeline.\")\n",
    "print(\"\\nThis workflow demonstrates:\")\n",
    "print(\"- Triplet learning for feature extraction\")\n",
    "print(\"- Standard classification on learned features\") \n",
    "print(\"- Semantic classification with different embedding scaling strategies\")\n",
    "print(\"- Comparison of different approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fa7e5",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization\n",
    "\n",
    "This section provides tools for analyzing and visualizing training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c570e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_training_results(results_file='triplet_training_results.csv'):\n",
    "    \"\"\"\n",
    "    Analyze and visualize training results from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        results_file: Path to the results CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load results\n",
    "        results_df = pd.read_csv(results_file)\n",
    "        print(f\"Loaded {len(results_df)} training results\")\n",
    "        \n",
    "        # Display basic statistics\n",
    "        print(\"\\n=== Training Results Summary ===\")\n",
    "        print(results_df.describe())\n",
    "        \n",
    "        # Group by dataset and model\n",
    "        print(\"\\n=== Results by Dataset and Model ===\")\n",
    "        grouped = results_df.groupby(['dataset', 'model']).agg({\n",
    "            'accuracy': ['mean', 'std'],\n",
    "            'precision': ['mean', 'std'],\n",
    "            'recall': ['mean', 'std'],\n",
    "            'f1': ['mean', 'std']\n",
    "        }).round(4)\n",
    "        print(grouped)\n",
    "        \n",
    "        # Create visualizations\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        if 'accuracy' in results_df.columns:\n",
    "            sns.boxplot(data=results_df, x='model', y='accuracy')\n",
    "            plt.title('Accuracy by Model')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        # F1 Score comparison\n",
    "        plt.subplot(2, 2, 2)\n",
    "        if 'f1' in results_df.columns:\n",
    "            sns.boxplot(data=results_df, x='model', y='f1')\n",
    "            plt.title('F1 Score by Model')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        # Loss comparison\n",
    "        plt.subplot(2, 2, 3)\n",
    "        if 'avg_test_loss' in results_df.columns:\n",
    "            sns.boxplot(data=results_df, x='model', y='avg_test_loss')\n",
    "            plt.title('Test Loss by Model')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        # Dataset performance\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if 'accuracy' in results_df.columns:\n",
    "            sns.boxplot(data=results_df, x='dataset', y='accuracy')\n",
    "            plt.title('Accuracy by Dataset')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results_df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Results file {results_file} not found. Run training first to generate results.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing results: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_training_metrics(metrics_dict):\n",
    "    \"\"\"\n",
    "    Plot training metrics over time.\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict: Dictionary containing lists of metrics (loss, accuracy, etc.)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot loss\n",
    "    if 'loss' in metrics_dict:\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(metrics_dict['loss'])\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "    \n",
    "    # Plot validation loss\n",
    "    if 'val_loss' in metrics_dict:\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(metrics_dict['val_loss'])\n",
    "        plt.title('Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "    \n",
    "    # Plot accuracy\n",
    "    if 'accuracy' in metrics_dict:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(metrics_dict['accuracy'])\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "    \n",
    "    # Plot combined losses\n",
    "    if 'loss' in metrics_dict and 'val_loss' in metrics_dict:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(metrics_dict['loss'], label='Training Loss')\n",
    "        plt.plot(metrics_dict['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss Comparison')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Results analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_training_methods(results_dict):\n",
    "    \"\"\"\n",
    "    Compare different training methods and visualize their performance.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary with method names as keys and results as values\n",
    "                     Each result should be a tuple of (model, avg_loss, test_loss, embeddings)\n",
    "    \"\"\"\n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for method, (model, avg_loss, test_loss, embeddings) in results_dict.items():\n",
    "        comparison_data.append({\n",
    "            'Method': method,\n",
    "            'Model': model._get_name() if hasattr(model, '_get_name') else str(type(model).__name__),\n",
    "            'Average Loss': avg_loss,\n",
    "            'Test Loss': test_loss,\n",
    "            'Has Embeddings': embeddings is not None\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"=== Training Methods Comparison ===\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Average Loss comparison\n",
    "    axes[0].bar(comparison_df['Method'], comparison_df['Average Loss'])\n",
    "    axes[0].set_title('Average Training Loss by Method')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Test Loss comparison\n",
    "    axes[1].bar(comparison_df['Method'], comparison_df['Test Loss'])\n",
    "    axes[1].set_title('Test Loss by Method')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "def visualize_semantic_embeddings(label_embeddings, title=\"Label Embeddings Visualization\"):\n",
    "    \"\"\"\n",
    "    Visualize label embeddings using dimensionality reduction.\n",
    "    \n",
    "    Args:\n",
    "        label_embeddings: Dictionary mapping labels to their embeddings\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    if not label_embeddings:\n",
    "        print(\"No label embeddings provided for visualization\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.manifold import TSNE\n",
    "        \n",
    "        # Prepare data\n",
    "        labels = list(label_embeddings.keys())\n",
    "        embeddings = torch.stack(list(label_embeddings.values())).detach().cpu().numpy()\n",
    "        \n",
    "        # Apply PCA for dimensionality reduction\n",
    "        if embeddings.shape[1] > 2:\n",
    "            pca = PCA(n_components=2)\n",
    "            embeddings_2d = pca.fit_transform(embeddings)\n",
    "            explained_variance = pca.explained_variance_ratio_.sum()\n",
    "        else:\n",
    "            embeddings_2d = embeddings\n",
    "            explained_variance = 1.0\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # PCA plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                            c=range(len(labels)), cmap='tab10', s=100)\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        plt.title(f'{title} (PCA)\\nExplained Variance: {explained_variance:.2%}')\n",
    "        plt.xlabel('First Principal Component')\n",
    "        plt.ylabel('Second Principal Component')\n",
    "        \n",
    "        # t-SNE plot (if we have enough samples)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if len(labels) > 3 and embeddings.shape[1] > 2:\n",
    "            try:\n",
    "                tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(labels)-1))\n",
    "                embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "                scatter = plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], \n",
    "                                    c=range(len(labels)), cmap='tab10', s=100)\n",
    "                for i, label in enumerate(labels):\n",
    "                    plt.annotate(label, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]), \n",
    "                                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "                plt.title(f'{title} (t-SNE)')\n",
    "                plt.xlabel('t-SNE 1')\n",
    "                plt.ylabel('t-SNE 2')\n",
    "            except Exception as e:\n",
    "                plt.text(0.5, 0.5, f't-SNE not available\\n{str(e)}', \n",
    "                        ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                plt.title('t-SNE (Not Available)')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'Not enough samples\\nfor t-SNE', \n",
    "                    ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('t-SNE (Not Available)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"scikit-learn not available for dimensionality reduction\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in visualization: {e}\")\n",
    "\n",
    "\n",
    "def performance_summary(trained_models_dict):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive performance summary for multiple trained models.\n",
    "    \n",
    "    Args:\n",
    "        trained_models_dict: Dictionary with model names as keys and results as values\n",
    "    \"\"\"\n",
    "    print(\"=== Performance Summary ===\")\n",
    "    \n",
    "    for name, results in trained_models_dict.items():\n",
    "        model, avg_loss, test_loss, embeddings = results\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Model Type: {model._get_name() if hasattr(model, '_get_name') else type(model).__name__}\")\n",
    "        print(f\"  Average Training Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"  Semantic Embeddings: {'Yes' if embeddings else 'No'}\")\n",
    "        if embeddings:\n",
    "            print(f\"  Number of Label Embeddings: {len(embeddings)}\")\n",
    "    \n",
    "    # Generate comparison visualizations\n",
    "    comparison_df = compare_training_methods(trained_models_dict)\n",
    "    \n",
    "    # Visualize embeddings for semantic methods\n",
    "    for name, results in trained_models_dict.items():\n",
    "        _, _, _, embeddings = results\n",
    "        if embeddings:\n",
    "            visualize_semantic_embeddings(embeddings, f\"Semantic Embeddings - {name}\")\n",
    "\n",
    "print(\"Model comparison and visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd24b7",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "To use this notebook effectively:\n",
    "\n",
    "1. **Setup**: Run the import and setup cells to initialize the environment\n",
    "2. **Configure Dataset**: Choose or create dataset configuration functions for your data\n",
    "3. **Select Model**: Choose a backbone model (ResNet, VGG, MobileNet, DenseNet)\n",
    "4. **Choose Training Process**:\n",
    "   - `'triplet'`: For learning feature embeddings using triplet loss\n",
    "   - `'classification'`: For standard supervised classification\n",
    "   - `'semantic_classification'`: For classification with semantic label embeddings and k-nearest neighbor validation\n",
    "5. **Execute Training**: Run the training function with your chosen parameters\n",
    "6. **Analyze Results**: Use the analysis and comparison functions to visualize and compare results\n",
    "\n",
    "### Example Usage:\n",
    "```python\n",
    "# 1. Create dataset\n",
    "dataset = create_gi4e_raw_eyes_classification_dataset()\n",
    "\n",
    "# 2. Create model\n",
    "model = FeatureExtractor(torchvision.models.resnet50(weights=None))\n",
    "\n",
    "# 3. Train with different approaches\n",
    "results = {}\n",
    "\n",
    "# Standard classification\n",
    "results['Standard'] = train(dataset, model, train_process='classification', k_fold=5, batch_size=32)\n",
    "\n",
    "# Semantic classification with linear scaling\n",
    "results['Semantic_Linear'] = train(dataset, model, train_process='semantic_classification', \n",
    "                                  semantic_embedding_fn=lambda x: x, k_fold=5, batch_size=32)\n",
    "\n",
    "# Semantic classification with quadratic scaling  \n",
    "results['Semantic_Quadratic'] = train(dataset, model, train_process='semantic_classification',\n",
    "                                     semantic_embedding_fn=lambda x: 4*x, k_fold=5, batch_size=32)\n",
    "\n",
    "# 4. Compare and analyze results\n",
    "comparison_df = compare_training_methods(results)\n",
    "performance_summary(results)\n",
    "```\n",
    "\n",
    "### Advanced Features:\n",
    "\n",
    "**Active Learning:**\n",
    "```python\n",
    "# Demonstrate active learning sample selection\n",
    "selected_samples, al_results = active_learning_demo(\n",
    "    trained_model, train_dataset, val_dataset, label_embeddings,\n",
    "    num_iterations=3, samples_per_iteration=5\n",
    ")\n",
    "```\n",
    "\n",
    "**Semantic Embeddings Visualization:**\n",
    "```python\n",
    "# Visualize label embeddings in 2D space\n",
    "visualize_semantic_embeddings(label_embeddings, \"My Label Embeddings\")\n",
    "```\n",
    "\n",
    "## Key Improvements in This Updated Version\n",
    "\n",
    "### 1. **Complete Semantic Classification Implementation**\n",
    "- Full `semantic_classification_train_process` function with k-fold cross-validation\n",
    "- Advanced `semantic_validate_model` with k-nearest neighbor centroid calculation\n",
    "- Proper integration of BERT/Nomic embeddings for label semantic representation\n",
    "\n",
    "### 2. **Active Learning Capabilities**\n",
    "- `get_n_most_informative_samples_indices` for uncertainty-based sample selection\n",
    "- `find_k_nearest_semantic_embeddings` for intelligent neighbor finding\n",
    "- Complete active learning demonstration workflow\n",
    "\n",
    "### 3. **Enhanced Analysis and Visualization**\n",
    "- Comprehensive model comparison functions\n",
    "- Label embeddings visualization with PCA and t-SNE\n",
    "- Performance summary with multiple metrics\n",
    "- Results tracking and CSV export functionality\n",
    "\n",
    "### 4. **Robust Training Pipeline**\n",
    "- Support for multiple semantic embedding scaling functions\n",
    "- Proper error handling and validation\n",
    "- Model saving/loading with organized directory structure\n",
    "- Cross-validation with detailed metrics (precision, recall, F1, accuracy)\n",
    "\n",
    "### 5. **Advanced Validation Techniques**\n",
    "- Semantic validation using k-nearest neighbors\n",
    "- Centroid-based embedding adjustments\n",
    "- Multi-metric evaluation framework\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This updated notebook provides a state-of-the-art framework for computer vision research and experimentation. The implementation now supports:\n",
    "\n",
    "- **Multiple datasets**: gi4e, CelebA, YouTube Faces, FER2013, etc.\n",
    "- **Various architectures**: ResNet, VGG, MobileNet, DenseNet\n",
    "- **Training strategies**: Standard classification, triplet learning, semantic embeddings with various scaling functions\n",
    "- **Advanced techniques**: Active learning, reinforcement learning, k-nearest neighbor validation\n",
    "- **Comprehensive evaluation**: Cross-validation, multiple metrics, visualization, comparison tools\n",
    "- **Research capabilities**: Semantic embedding analysis, uncertainty estimation, intelligent sample selection\n",
    "\n",
    "The modular design and comprehensive implementation make this ideal for:\n",
    "- **Research**: Comparing different computer vision training methodologies\n",
    "- **Education**: Understanding the impact of semantic embeddings and active learning\n",
    "- **Development**: Building production-ready computer vision systems\n",
    "- **Experimentation**: Testing novel approaches and scaling functions\n",
    "\n",
    "Key advantages of this approach:\n",
    "1. **Semantic Understanding**: Uses BERT embeddings to capture semantic relationships between labels\n",
    "2. **Intelligent Training**: Active learning reduces annotation requirements\n",
    "3. **Robust Validation**: K-nearest neighbor validation provides better generalization assessment  \n",
    "4. **Comprehensive Analysis**: Multiple visualization and comparison tools for deep insights\n",
    "5. **Scalable Framework**: Easy to extend with new datasets, models, and training strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
