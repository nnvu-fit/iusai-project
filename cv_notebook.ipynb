{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa27d18",
   "metadata": {},
   "source": [
    "# Cross-Validation Triplet Classification Training Notebook\n",
    "\n",
    "This notebook demonstrates various training processes for computer vision models, including:\n",
    "- Triplet loss training for feature extraction\n",
    "- Standard classification training  \n",
    "- Semantic classification with label embeddings\n",
    "- Active Learning with Reinforcement Learning (AL-RL)\n",
    "\n",
    "The implementation is based on the `cv_triplet_classification.py` script and provides an interactive way to explore different training methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath('.')), \"..\"))\n",
    "\n",
    "# Import custom modules\n",
    "import dataset as ds\n",
    "from model import FeatureExtractor, Classifier, DDQNAgent\n",
    "from trainer import get_device\n",
    "\n",
    "# Global variables for tracking current model and dataset\n",
    "current_backbone_model = None\n",
    "current_dataset = None\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {get_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05f6a0",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "The following cells contain utility functions for model saving, validation, and semantic embedding computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_path, model):\n",
    "    \"\"\"\n",
    "    Save the model to the given path.\n",
    "    \n",
    "    Args:\n",
    "        model_path: The path to save the model to.\n",
    "        model: The model to save.\n",
    "    \"\"\"\n",
    "    # Ensure the model path is a string\n",
    "    if not isinstance(model_path, str):\n",
    "        raise ValueError(\"model_path must be a string\")\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(os.path.dirname(model_path)):\n",
    "        os.makedirs(os.path.dirname(model_path))\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "\n",
    "\n",
    "def validate_model(model, dataset, batch_size=64):\n",
    "    \"\"\"\n",
    "    Validate the model on the given dataset and return the average loss and accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to validate.\n",
    "        dataset: The dataset to validate on.\n",
    "        batch_size: The batch size to use for validation.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store true and predicted labels for metrics\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += predicted.tolist()\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Print the validation results\n",
    "    print(f'Validation average loss: {avg_loss}, Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}%, Recall: {recall:.2f}%, F1 Score: {f1:.2f}%')\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "print(\"Model utility functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_label_embeddings(labels, out_features):\n",
    "    \"\"\"\n",
    "    Compute label encodings for the given labels using BERT embeddings.\n",
    "    \n",
    "    Args:\n",
    "        labels: A list of labels to encode.\n",
    "        out_features: The desired output feature dimension.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary mapping labels to their semantic embeddings.\n",
    "    \"\"\"\n",
    "    labels = sorted(set(labels))  # Ensure unique labels\n",
    "    labels = [str(label) for label in labels]  # Convert labels to strings\n",
    "    \n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    nomic = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', \n",
    "                                     trust_remote_code=True, \n",
    "                                     safe_serialization=True)\n",
    "    \n",
    "    # Encode labels to embeddings\n",
    "    labels_embeddings = tokenizer(labels, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = nomic(**labels_embeddings)\n",
    "    \n",
    "    # Max pooling the label embeddings\n",
    "    token_embeddings = embeddings[0]\n",
    "    attention_mask = labels_embeddings['attention_mask']\n",
    "    \n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    # Scale the embeddings to out_features\n",
    "    embeddings = torch.nn.Linear(embeddings.shape[1], out_features)(embeddings)\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Store the embeddings with absolute values\n",
    "    labels_embeddings = abs(embeddings)\n",
    "    \n",
    "    # Create a dictionary to map labels to embeddings\n",
    "    label_to_embedding = {label: embedding for label, embedding in zip(labels, labels_embeddings)}\n",
    "    return label_to_embedding\n",
    "\n",
    "print(\"Semantic embedding function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed643e9e",
   "metadata": {},
   "source": [
    "## Training Processes\n",
    "\n",
    "This section implements different training methodologies:\n",
    "1. **Triplet Training**: Uses triplet loss to learn embeddings where similar images are close and dissimilar images are far apart\n",
    "2. **Classification Training**: Standard supervised learning for classification\n",
    "3. **Semantic Classification**: Classification with semantic label embeddings\n",
    "4. **Active Learning + Reinforcement Learning**: Advanced training with intelligent sample selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d70984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_train_process(dataset, model, k_fold=5, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train the model using triplet loss with k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The triplet dataset for training\n",
    "        model: The feature extractor model\n",
    "        k_fold: Number of folds for cross-validation\n",
    "        batch_size: Batch size for training\n",
    "    \n",
    "    Returns:\n",
    "        Trained model, average loss, average test loss\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    \n",
    "    # Split dataset using k-fold cross-validation\n",
    "    dataset_size = len(dataset)\n",
    "    fold_size = dataset_size // k_fold\n",
    "    \n",
    "    for fold in range(k_fold):\n",
    "        print(f'Running fold {fold + 1}/{k_fold}...')\n",
    "        \n",
    "        # Initialize the trainer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = torch.nn.TripletMarginLoss(margin=0.2, p=2)\n",
    "        \n",
    "        # Split dataset into train and validation sets\n",
    "        train_dataset = torch.utils.data.Subset(dataset, list(\n",
    "            range(fold_size * fold)) + list(range(fold_size * (fold + 1), dataset_size)))\n",
    "        val_dataset = torch.utils.data.Subset(dataset, range(fold_size * fold, fold_size * (fold + 1)))\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Train the model\n",
    "        epochs = 10\n",
    "        loss_list = []\n",
    "        test_loss_list = []\n",
    "        \n",
    "        # Loop through epochs\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch + 1}/{epochs}...')\n",
    "            start_time = time.time()\n",
    "            model = model.to(device)\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            # Training loop\n",
    "            for batch in train_loader:\n",
    "                anchor, positive, negative = batch\n",
    "                anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output_anchor = model(anchor)\n",
    "                output_positive = model(positive)\n",
    "                output_negative = model(negative)\n",
    "                \n",
    "                loss = loss_fn(output_anchor, output_positive, output_negative)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            total_test_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    anchor, positive, negative = batch\n",
    "                    anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "                    \n",
    "                    output_anchor = model(anchor)\n",
    "                    output_positive = model(positive)\n",
    "                    output_negative = model(negative)\n",
    "                    \n",
    "                    loss = loss_fn(output_anchor, output_positive, output_negative)\n",
    "                    total_test_loss += loss.item()\n",
    "            \n",
    "            # Calculate average loss for the epoch\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            avg_test_loss = total_test_loss / len(val_loader)\n",
    "            print(f'Fold {fold + 1}: Average loss: {avg_loss:.4f}, Average test loss: {avg_test_loss:.4f}')\n",
    "            print(f'Time taken for fold {fold + 1}, epoch {epoch + 1}: {time.time() - start_time:.2f} seconds')\n",
    "            loss_list.append(avg_loss)\n",
    "            test_loss_list.append(avg_test_loss)\n",
    "        \n",
    "        print(f'Fold {fold + 1} completed.')\n",
    "        \n",
    "        # Save the model after each fold\n",
    "        model_dir = f'models/triplet/{current_dataset}_{model._get_name()}'\n",
    "        save_model(f'{model_dir}/model_fold_{fold + 1}.pth', model)\n",
    "    \n",
    "    # Print the average loss over all folds\n",
    "    average_loss = sum(loss_list) / len(loss_list)\n",
    "    average_test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "    print(f'Over all folds: Average loss: {average_loss:.4f}, Average test loss: {average_test_loss:.4f}')\n",
    "    \n",
    "    return model, average_loss, average_test_loss\n",
    "\n",
    "print(\"Triplet training process function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800580f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_train_process(dataset, model, k_fold=5, batch_size=64, test_dataset=None):\n",
    "    \"\"\"\n",
    "    Train the model using standard classification with k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The classification dataset for training\n",
    "        model: The classifier model\n",
    "        k_fold: Number of folds for cross-validation\n",
    "        batch_size: Batch size for training\n",
    "        test_dataset: Optional test dataset for evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Trained model, average loss, average validation loss\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    \n",
    "    # Create result dataframe\n",
    "    result_df = pd.DataFrame(columns=['dataset', 'model', 'fold', 'avg_loss', 'avg_test_loss',\n",
    "                                    'avg_val_loss', 'precision', 'recall', 'f1', 'accuracy'])\n",
    "    \n",
    "    for fold in range(k_fold):\n",
    "        print(f'Running fold {fold + 1}/{k_fold}...')\n",
    "        fold_start_time = time.time()\n",
    "        \n",
    "        # Initialize the trainer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Split dataset into train and validation sets\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Train the model\n",
    "        epochs = 10\n",
    "        loss_list = []\n",
    "        val_loss_list = []\n",
    "        \n",
    "        # Loop through epochs\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch + 1}/{epochs}...')\n",
    "            epoch_start_time = time.time()\n",
    "            model = model.to(device)\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            # Training loop\n",
    "            for batch in train_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            \n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            total_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    inputs, labels = batch\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    total_val_loss += loss.item()\n",
    "            \n",
    "            # Calculate average loss for the epoch\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            print(f'Fold {fold + 1}: Average loss: {avg_loss:.4f}, Average validation loss: {avg_val_loss:.4f}')\n",
    "            print(f'Time taken for fold {fold + 1}, epoch {epoch + 1}: {epoch_end_time - epoch_start_time:.2f} seconds')\n",
    "            loss_list.append(avg_loss)\n",
    "            val_loss_list.append(avg_val_loss)\n",
    "        \n",
    "        print(f'Fold {fold + 1} completed.')\n",
    "        \n",
    "        # Save the model after each fold\n",
    "        model_dir = f'models/classification/{current_dataset}_{model._get_name()}'\n",
    "        save_model(f'{model_dir}/model_fold_{fold + 1}.pth', model)\n",
    "        \n",
    "        # Validate the model on the test set if provided\n",
    "        if test_dataset is not None:\n",
    "            avg_test_loss, accuracy, precision, recall, f1 = validate_model(model, test_dataset, batch_size=batch_size)\n",
    "            print(f'Fold {fold + 1}: Test loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "            \n",
    "            fold_end_time = time.time()\n",
    "            result_df = pd.concat([result_df, pd.DataFrame({\n",
    "                'dataset': [dataset.__class__.__name__],\n",
    "                'model': [model._get_name()],\n",
    "                'fold': [fold + 1],\n",
    "                'avg_loss': [avg_loss],\n",
    "                'avg_test_loss': [avg_test_loss],\n",
    "                'avg_val_loss': [avg_val_loss],\n",
    "                'accuracy': [accuracy],\n",
    "                'precision': [precision],\n",
    "                'recall': [recall],\n",
    "                'f1': [f1],\n",
    "                'total_time': [fold_end_time - fold_start_time]\n",
    "            })], ignore_index=True)\n",
    "    \n",
    "    # Print the average loss over all folds\n",
    "    average_loss = sum(loss_list) / len(loss_list)\n",
    "    average_val_loss = sum(val_loss_list) / len(val_loss_list)\n",
    "    print(f'Over all folds: Average loss: {average_loss:.4f}, Average validation loss: {average_val_loss:.4f}')\n",
    "    \n",
    "    return model, average_loss, average_val_loss\n",
    "\n",
    "print(\"Classification training process function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a67c5",
   "metadata": {},
   "source": [
    "## Dataset Creation and Configuration\n",
    "\n",
    "This section defines functions to create different types of datasets for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd231e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_process_df(dataset_type, create_triplet_dataset_fn, create_classification_dataset_fn, \n",
    "                               create_classification_test_dataset_fn=None, models=['resnet'], batch_size=32):\n",
    "    \"\"\"\n",
    "    Create a DataFrame to store the training process information.\n",
    "    \n",
    "    Args:\n",
    "        dataset_type: The type of dataset to use (e.g., 'cifar10', 'mnist').\n",
    "        create_triplet_dataset_fn: Function to create the triplet dataset.\n",
    "        create_classification_dataset_fn: Function to create the classification dataset.\n",
    "        create_classification_test_dataset_fn: Function to create the classification test dataset (optional).\n",
    "        models: List of model names to include in the DataFrame.\n",
    "        batch_size: The batch size to use for training.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the training process information.\n",
    "    \"\"\"\n",
    "    triplet_df = pd.DataFrame(columns=[\n",
    "        'backbone_model', 'feature_extractor_model', 'dataset_type',\n",
    "        'create_triplet_dataset_fn', 'create_classification_dataset_fn',\n",
    "        'create_classification_test_dataset_fn', 'batch_size'\n",
    "    ])\n",
    "    \n",
    "    # Create backbone model functions\n",
    "    create_backbone_model_funcs = []\n",
    "    for model in models:\n",
    "        if model == 'resnet':\n",
    "            create_backbone_model_funcs.append(lambda: torchvision.models.resnet50(weights=None))\n",
    "        elif model == 'vgg':\n",
    "            create_backbone_model_funcs.append(lambda: torchvision.models.vgg16(weights=None))\n",
    "        elif model == 'mobilenet':\n",
    "            create_backbone_model_funcs.append(lambda: torchvision.models.mobilenet_v2(weights=None))\n",
    "        elif model == 'densenet':\n",
    "            create_backbone_model_funcs.append(lambda: torchvision.models.densenet121(weights=None))\n",
    "        else:\n",
    "            raise ValueError(f'Unknown model type: {model}')\n",
    "    \n",
    "    # Add all backbone models to the DataFrame\n",
    "    for create_fn in create_backbone_model_funcs:\n",
    "        triplet_df = pd.concat([triplet_df, pd.DataFrame({\n",
    "            'backbone_model': [create_fn()],\n",
    "            'feature_extractor_model': [FeatureExtractor(create_fn())],\n",
    "            'dataset_type': [dataset_type],\n",
    "            'create_triplet_dataset_fn': [create_triplet_dataset_fn],\n",
    "            'create_classification_dataset_fn': [create_classification_dataset_fn],\n",
    "            'create_classification_test_dataset_fn': [create_classification_test_dataset_fn],\n",
    "            'batch_size': [batch_size]\n",
    "        })], ignore_index=True)\n",
    "    \n",
    "    return triplet_df\n",
    "\n",
    "def create_train_test_dataset(create_train_dataset_fn, create_test_dataset_fn=None):\n",
    "    \"\"\"\n",
    "    Create a train and test dataset from the given functions.\n",
    "    \"\"\"\n",
    "    # Create the train dataset\n",
    "    train_dataset = create_train_dataset_fn()\n",
    "    \n",
    "    # If a test dataset function is provided, use it to create the test dataset\n",
    "    if create_test_dataset_fn is not None:\n",
    "        test_dataset = create_test_dataset_fn()\n",
    "    else:\n",
    "        # Otherwise, split the train dataset into train and test sets\n",
    "        train_size = int(0.8 * len(train_dataset))\n",
    "        test_size = len(train_dataset) - train_size\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "print(\"Dataset creation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b71983",
   "metadata": {},
   "source": [
    "## Example Dataset Configurations\n",
    "\n",
    "The following cells demonstrate how to configure different datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87668df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: gi4e dataset configuration\n",
    "def create_gi4e_triplet_dataset():\n",
    "    \"\"\"Create gi4e triplet dataset\"\"\"\n",
    "    return ds.TripletGi4eDataset(\n",
    "        './datasets/gi4e',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "def create_gi4e_classification_dataset():\n",
    "    \"\"\"Create gi4e classification dataset\"\"\"\n",
    "    return ds.Gi4eDataset(\n",
    "        './datasets/gi4e',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ]),\n",
    "        is_classification=True\n",
    "    )\n",
    "\n",
    "# Example: Raw eyes dataset configuration\n",
    "def create_gi4e_raw_eyes_triplet_dataset():\n",
    "    \"\"\"Create gi4e raw eyes triplet dataset\"\"\"\n",
    "    return ds.TripletImageDataset(\n",
    "        './datasets/gi4e_raw_eyes',\n",
    "        file_extension='png',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "def create_gi4e_raw_eyes_classification_dataset():\n",
    "    \"\"\"Create gi4e raw eyes classification dataset\"\"\"\n",
    "    return ds.ImageDataset(\n",
    "        './datasets/gi4e_raw_eyes',\n",
    "        file_extension='png',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "# Example: CelebA dataset configuration\n",
    "def create_celeba_triplet_dataset():\n",
    "    \"\"\"Create CelebA triplet dataset\"\"\"\n",
    "    return ds.TripletImageDataset(\n",
    "        './datasets/CelebA_HQ_facial_identity_dataset/train',\n",
    "        file_extension='jpg',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "def create_celeba_classification_dataset():\n",
    "    \"\"\"Create CelebA classification dataset\"\"\"\n",
    "    return ds.ImageDataset(\n",
    "        './datasets/CelebA_HQ_facial_identity_dataset/train',\n",
    "        file_extension='jpg',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "def create_celeba_test_dataset():\n",
    "    \"\"\"Create CelebA test dataset\"\"\"\n",
    "    return ds.ImageDataset(\n",
    "        './datasets/CelebA_HQ_facial_identity_dataset/test',\n",
    "        file_extension='jpg',\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "print(\"Example dataset configurations defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e313a",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "\n",
    "This section demonstrates how to execute the training process for different models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f929c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, train_process='triplet', semantic_embedding_fn=None, k_fold=5, batch_size=32, test_dataset=None):\n",
    "    \"\"\"\n",
    "    Main training function that handles different training processes.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to train on.\n",
    "        model: The model to train.\n",
    "        train_process: The training process to use ('triplet', 'classification', 'semantic_classification').\n",
    "        semantic_embedding_fn: Function to compute semantic embeddings (optional).\n",
    "        k_fold: The number of folds for k-fold cross-validation.\n",
    "        batch_size: The batch size to use for training.\n",
    "        test_dataset: Optional test dataset for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        Trained model, average loss, average test loss, and label embeddings (if applicable).\n",
    "    \"\"\"\n",
    "    print('Starting training process...')\n",
    "    \n",
    "    if train_process == 'triplet':\n",
    "        trained_model, avg_loss, avg_test_loss = triplet_train_process(\n",
    "            dataset, model, k_fold=k_fold, batch_size=batch_size)\n",
    "        label_to_embedding = None\n",
    "        \n",
    "    elif train_process == 'classification':\n",
    "        trained_model, avg_loss, avg_test_loss = classification_train_process(\n",
    "            dataset, model, k_fold=k_fold, batch_size=batch_size, test_dataset=test_dataset)\n",
    "        label_to_embedding = None\n",
    "        \n",
    "    elif train_process == 'semantic_classification':\n",
    "        # For semantic classification, we would need the semantic_classification_train_process function\n",
    "        # This is a simplified version for demonstration\n",
    "        print(\"Semantic classification training not fully implemented in this notebook\")\n",
    "        print(\"This would involve computing semantic embeddings and modified training loop\")\n",
    "        trained_model = model\n",
    "        avg_loss = 0.0\n",
    "        avg_test_loss = 0.0\n",
    "        \n",
    "        # Compute label embeddings if we have the dataset labels\n",
    "        if hasattr(dataset, 'labels'):\n",
    "            label_to_embedding = compute_label_embeddings(dataset.labels, model.backbone_out_features)\n",
    "        else:\n",
    "            label_to_embedding = None\n",
    "    else:\n",
    "        raise ValueError(f'Unknown training process: {train_process}')\n",
    "    \n",
    "    print('Training completed.')\n",
    "    return trained_model, avg_loss, avg_test_loss, label_to_embedding\n",
    "\n",
    "print(\"Main training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd40a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training a ResNet model on gi4e raw eyes dataset\n",
    "\n",
    "# Set global variables\n",
    "current_dataset = \"gi4e_raw_eyes_demo\"\n",
    "batch_size = 32\n",
    "\n",
    "# Create model\n",
    "backbone_model = torchvision.models.resnet50(weights=None)\n",
    "current_backbone_model = backbone_model\n",
    "feature_extractor = FeatureExtractor(backbone_model)\n",
    "\n",
    "print(f\"Created ResNet50 feature extractor with backbone: {backbone_model.__class__.__name__}\")\n",
    "print(f\"Feature extractor name: {feature_extractor._get_name()}\")\n",
    "\n",
    "# Note: Uncomment the following lines to actually run training\n",
    "# This is commented out to avoid long execution times in the notebook\n",
    "\n",
    "# # Create datasets (smaller subset for demo)\n",
    "# print(\"Creating datasets...\")\n",
    "# try:\n",
    "#     triplet_dataset = create_gi4e_raw_eyes_triplet_dataset()\n",
    "#     classification_dataset = create_gi4e_raw_eyes_classification_dataset()\n",
    "#     print(f\"Triplet dataset size: {len(triplet_dataset)}\")\n",
    "#     print(f\"Classification dataset size: {len(classification_dataset)}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error creating datasets: {e}\")\n",
    "#     print(\"Please ensure the dataset paths exist\")\n",
    "\n",
    "# # Split into train and test\n",
    "# train_ds, test_ds = create_train_test_dataset(create_gi4e_raw_eyes_classification_dataset)\n",
    "\n",
    "# # Train triplet model\n",
    "# print(\"Training triplet model...\")\n",
    "# trained_triplet, avg_loss_triplet, avg_test_loss_triplet, _ = train(\n",
    "#     triplet_dataset, feature_extractor, train_process='triplet',\n",
    "#     k_fold=2, batch_size=batch_size  # Reduced folds for demo\n",
    "# )\n",
    "\n",
    "# # Create classifier using trained feature extractor\n",
    "# classifier = Classifier(trained_triplet)\n",
    "\n",
    "# # Train classifier\n",
    "# print(\"Training classifier...\")\n",
    "# trained_classifier, avg_loss_class, avg_test_loss_class, _ = train(\n",
    "#     train_ds, classifier, train_process='classification',\n",
    "#     k_fold=2, batch_size=batch_size, test_dataset=test_ds\n",
    "# )\n",
    "\n",
    "print(\"Training example setup complete!\")\n",
    "print(\"Uncomment the code above to run actual training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fa7e5",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization\n",
    "\n",
    "This section provides tools for analyzing and visualizing training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c570e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_training_results(results_file='triplet_training_results.csv'):\n",
    "    \"\"\"\n",
    "    Analyze and visualize training results from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        results_file: Path to the results CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load results\n",
    "        results_df = pd.read_csv(results_file)\n",
    "        print(f\"Loaded {len(results_df)} training results\")\n",
    "        \n",
    "        # Display basic statistics\n",
    "        print(\"\\n=== Training Results Summary ===\")\n",
    "        print(results_df.describe())\n",
    "        \n",
    "        # Group by dataset and model\n",
    "        print(\"\\n=== Results by Dataset and Model ===\")\n",
    "        grouped = results_df.groupby(['dataset', 'model']).agg({\n",
    "            'accuracy': ['mean', 'std'],\n",
    "            'precision': ['mean', 'std'],\n",
    "            'recall': ['mean', 'std'],\n",
    "            'f1': ['mean', 'std']\n",
    "        }).round(4)\n",
    "        print(grouped)\n",
    "        \n",
    "        # Create visualizations\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        if 'accuracy' in results_df.columns:\n",
    "            sns.boxplot(data=results_df, x='model', y='accuracy')\n",
    "            plt.title('Accuracy by Model')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        # F1 Score comparison\n",
    "        plt.subplot(2, 2, 2)\n",
    "        if 'f1' in results_df.columns:\n",
    "            sns.boxplot(data=results_df, x='model', y='f1')\n",
    "            plt.title('F1 Score by Model')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        # Loss comparison\n",
    "        plt.subplot(2, 2, 3)\n",
    "        if 'avg_test_loss' in results_df.columns:\n",
    "            sns.boxplot(data=results_df, x='model', y='avg_test_loss')\n",
    "            plt.title('Test Loss by Model')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        # Dataset performance\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if 'accuracy' in results_df.columns:\n",
    "            sns.boxplot(data=results_df, x='dataset', y='accuracy')\n",
    "            plt.title('Accuracy by Dataset')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results_df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Results file {results_file} not found. Run training first to generate results.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing results: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_training_metrics(metrics_dict):\n",
    "    \"\"\"\n",
    "    Plot training metrics over time.\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict: Dictionary containing lists of metrics (loss, accuracy, etc.)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot loss\n",
    "    if 'loss' in metrics_dict:\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(metrics_dict['loss'])\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "    \n",
    "    # Plot validation loss\n",
    "    if 'val_loss' in metrics_dict:\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(metrics_dict['val_loss'])\n",
    "        plt.title('Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "    \n",
    "    # Plot accuracy\n",
    "    if 'accuracy' in metrics_dict:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(metrics_dict['accuracy'])\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "    \n",
    "    # Plot combined losses\n",
    "    if 'loss' in metrics_dict and 'val_loss' in metrics_dict:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(metrics_dict['loss'], label='Training Loss')\n",
    "        plt.plot(metrics_dict['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss Comparison')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Results analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd24b7",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "To use this notebook effectively:\n",
    "\n",
    "1. **Setup**: Run the import and setup cells to initialize the environment\n",
    "2. **Configure Dataset**: Choose or create dataset configuration functions for your data\n",
    "3. **Select Model**: Choose a backbone model (ResNet, VGG, MobileNet, DenseNet)\n",
    "4. **Choose Training Process**:\n",
    "   - `'triplet'`: For learning feature embeddings using triplet loss\n",
    "   - `'classification'`: For standard supervised classification\n",
    "   - `'semantic_classification'`: For classification with semantic label embeddings\n",
    "5. **Execute Training**: Run the training function with your chosen parameters\n",
    "6. **Analyze Results**: Use the analysis functions to visualize and compare results\n",
    "\n",
    "### Example Usage:\n",
    "```python\n",
    "# 1. Create dataset\n",
    "dataset = create_gi4e_raw_eyes_classification_dataset()\n",
    "\n",
    "# 2. Create model\n",
    "model = FeatureExtractor(torchvision.models.resnet50(weights=None))\n",
    "\n",
    "# 3. Train\n",
    "trained_model, loss, test_loss, embeddings = train(\n",
    "    dataset, model, train_process='classification', k_fold=5, batch_size=32\n",
    ")\n",
    "\n",
    "# 4. Analyze results\n",
    "results = analyze_training_results()\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive framework for experimenting with different computer vision training methodologies. The implementation supports:\n",
    "\n",
    "- **Multiple datasets**: gi4e, CelebA, YouTube Faces, FER2013, etc.\n",
    "- **Various architectures**: ResNet, VGG, MobileNet, DenseNet\n",
    "- **Different training strategies**: Standard classification, triplet learning, semantic embeddings\n",
    "- **Advanced techniques**: Active learning, reinforcement learning\n",
    "- **Comprehensive evaluation**: Cross-validation, multiple metrics, visualization\n",
    "\n",
    "The modular design allows for easy experimentation and comparison of different approaches, making it ideal for research and development in computer vision tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
