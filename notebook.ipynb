{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "## Outline\n",
    "1. Import Library\n",
    "2. Setup pre-requisites\n",
    "3. Extract dataset to images\n",
    "4. Upload images to Azure data store\n",
    "4. Setting up Azure ML Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install needed packages\n",
    "%pip install opencv-python torch torchvision torchaudio azureml-core scikit-learn tf2onnx onnx2pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx2pytorch import ConvertModel\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "import tf2onnx\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(96, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(24, activation='softmax')\n",
    "])\n",
    "# Convert the model to ONNX format\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model)\n",
    "\n",
    "ConvertModel(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "import cv2\n",
    "# from azureml.core import Workspace, Dataset, Datastore\n",
    "\n",
    "## Using torchvision to create a dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone source code from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## curl source code from github\n",
    "!wget 'https://github.com/nnvu-fit/iusai-project/archive/refs/heads/main.zip' -O main.zip\n",
    "!unzip main.zip\n",
    "!mv iusai-project-main/* .\n",
    "!rm -rf iusai-project-main main.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    }
   },
   "outputs": [],
   "source": [
    "## Setup Azure ML Workspace\n",
    "ws = Workspace.from_config()\n",
    "## From workspace, get/create the default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "ws, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract dataset to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "## define videos location + images output location\n",
    "video_path = 'videos/'\n",
    "images_path = 'subjects-small/'\n",
    "print('video_path: ', video_path)\n",
    "print('images_path: ', images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import v2i\n",
    "reload(v2i)\n",
    "from v2i import extract_images_from_videos\n",
    "import threading\n",
    "\n",
    "def extract_images_from_videos_collection(video_path, images_path):\n",
    "    ## get all videos file in video_path\n",
    "    video_list_location_collection = os.listdir(video_path)\n",
    "\n",
    "    ## define total_label_dict\n",
    "    ## create threading pool\n",
    "    thread = []\n",
    "    ## for each video file\n",
    "    for video_list_location in video_list_location_collection:\n",
    "        ## check if video_location is not a directory (i.e. is a file), then skip\n",
    "        if not os.path.isdir(video_path + video_list_location):\n",
    "            continue\n",
    "        ## list videos in video_location\n",
    "        thread.append(threading.Thread(target=extract_images_from_videos, args=(video_path + video_list_location, images_path, 1)))\n",
    "        # extract_images_from_videos(video_path + video_list_location, images_path, inteval=1)\n",
    "\n",
    "    ## start all threads\n",
    "    for t in thread: t.start()\n",
    "    ## wait for all threads to finish\n",
    "    for t in thread:\n",
    "        t.join()\n",
    "    \n",
    "    ## check images in images_path\n",
    "    images_list_location_collection = os.listdir(images_path)\n",
    "    print('images_list_location_collection: ', images_list_location_collection)\n",
    "\n",
    "# video_subject_6_path = video_path + '/subject6-###'\n",
    "# ## list videos in video_location\n",
    "# label_dict = extract_images_from_videos(video_subject_6_path, images_path)\n",
    "\n",
    "## extract images from videos\n",
    "extract_images_from_videos_collection(video_path, images_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Upload images to Azure data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "## upload images to data asset using Dataset.File.upload_directory\n",
    "Dataset.File.upload_directory(src_dir=images_path, target=(ds, 'images-extra-small'), overwrite=True, show_progress=True) \\\n",
    "    .register(workspace=ws, name='images-extra-small', description='images-extra-small') ## register dataset \\\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Setup public workspace endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_collection_list = ['images-extra-small', 'images-small-v2', 'images-medium']\n",
    "## register dataset using Dataset.File.from_files\n",
    "for dataset_collection in dataset_collection_list:\n",
    "    ## check if dataset_collection is already registered\n",
    "    if dataset_collection in ws.datasets.keys():\n",
    "        print('dataset_collection: ', dataset_collection, ' is already registered')\n",
    "        continue\n",
    "    Dataset.File.from_files(path=(ds, dataset_collection)) \\\n",
    "        .register(workspace=ws, name=dataset_collection, description=dataset_collection) ## register dataset \\\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train models\n",
    "1. ResNET\n",
    "2. DenseNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup device + load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "images_path = 'images'\n",
    "images_ds_path = \"images-extra-small\"\n",
    "if not os.path.exists(images_path):\n",
    "    os.mkdir(images_path)\n",
    "# download data asset to local if images_path is empty\n",
    "if len(os.listdir(images_path)) == 0:\n",
    "    print('images_path is empty, download images_ds to images_path')\n",
    "    # download data asset to local\n",
    "    images_ds = Dataset.get_by_name(workspace=ws, name=images_ds_path)\n",
    "    images_ds.download(target_path=images_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "#### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "imageDataset = ds.ImageDataset('datasets/subjects-small', transform=transform)\n",
    "## define batch_size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  ['subject1', 'subject10', 'subject11', 'subject12', 'subject13', 'subject14', 'subject15', 'subject16', 'subject17', 'subject18', 'subject19', 'subject2', 'subject20', 'subject21', 'subject22', 'subject23', 'subject24', 'subject3', 'subject4', 'subject5', 'subject6', 'subject7', 'subject8', 'subject9']\n"
     ]
    }
   ],
   "source": [
    "image, label = imageDataset.get_image(0)\n",
    "# image.show()\n",
    "labels = imageDataset.labels()\n",
    "## show labels in Interger\n",
    "print('labels: ',  labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val_ds:  22637\n",
      "test_ds:  2516\n"
     ]
    }
   ],
   "source": [
    "## split dataset into train and test dataset using random_split\n",
    "from torch.utils.data import random_split\n",
    "train_val_size = int(0.9 * len(imageDataset))\n",
    "test_size = len(imageDataset) - train_val_size\n",
    "train_val_ds, test_ds = random_split(imageDataset, [train_val_size, test_size])\n",
    "\n",
    "## split train_val_ds into train_ds and val_ds using random_split\n",
    "train_size = int(0.8 * len(train_val_ds))\n",
    "val_size = len(train_val_ds) - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(train_val_ds, [train_size, val_size])\n",
    "\n",
    "print('train_val_ds: ', len(train_val_ds))\n",
    "print('test_ds: ', len(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Resnes18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## import train and scrore function from train.py\n",
    "import train as t\n",
    "reload(t)\n",
    "from train import ClassifierTrainer\n",
    "\n",
    "## get resnet model of image classification from torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "## define optimizer using Adam and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer, loss_fn\n",
    "\n",
    "trainer = ClassifierTrainer(model, optimizer, loss_fn, random_seed_value=86)\n",
    "\n",
    "print('device: ', trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## train model\n",
    "avg_lost = trainer.cross_validate(train_val_ds, epochs=5)\n",
    "print('avg_lost: ', avg_lost)\n",
    "## score model\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "trainer.score(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## save model\n",
    "torch.save(model.state_dict(), 'model_resnes18.100_epochs.small.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Resnes34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get resnet model of image classification from torchvision\n",
    "model = torchvision.models.resnet34(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "## define optimizer using Adam and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model using device (CPU or GPU) 10 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=1, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "torch.save(model.state_dict(), 'model_resnes34.small.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get resnet model of image classification from torchvision\n",
    "model = torchvision.models.densenet121(pretrained=True)\n",
    "## define optimizer using Adam and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=1, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model using device (CPU or GPU) 10 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=100, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "torch.save(model.state_dict(), 'model_resnes18.100_epochs.small.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## get denseNet model of image classification from torchvision\n",
    "model = torchvision.models.densenet121(pretrained=True, num_classes=len(labels))\n",
    "## define optimizer using Adam and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "## train model using device (CPU or GPU) 1 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=1, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "## train model using device (CPU or GPU) 10 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=10, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "## save model\n",
    "torch.save(model.state_dict(), 'model_densenet121.small.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using Yolov5 to extract face then using ResNet to classification the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use yoloresnet model\n",
    "import model\n",
    "reload(model)\n",
    "from model import YoloResnet\n",
    "model = YoloResnet()\n",
    "## define optimizer using Adam and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model using device (CPU or GPU) 1 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=1, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model using device (CPU or GPU) 10 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=10, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "torch.save(model.state_dict(), 'model_yoloresnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "## load opencv haarcascade eye from xml file\n",
    "haarcasecade_eye_xml = './opencv/haarcascade_eye.xml'\n",
    "eye_cascade = cv2.CascadeClassifier(haarcasecade_eye_xml)\n",
    "\n",
    "## open camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    ## show frame with mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    ## convert frame to gray\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    ## detect eyes in frame\n",
    "    eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    for (x, y, w, h) in eyes:\n",
    "        ## draw rectangle around eyes\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    ## show frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:973: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 45\u001b[0m\n\u001b[0;32m     41\u001b[0m                 eye_image \u001b[38;5;241m=\u001b[39m frame[ey:ey\u001b[38;5;241m+\u001b[39meh, ex:ex\u001b[38;5;241m+\u001b[39mew]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m## show eye_image\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meye_image\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meye_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m## show frame\u001b[39;00m\n\u001b[0;32m     49\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:973: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## load opencv haarcascade eye from xml file\n",
    "haarcasecade_eye_xml = './opencv/haarcascade_eye.xml'\n",
    "eye_cascade = cv2.CascadeClassifier(haarcasecade_eye_xml)\n",
    "\n",
    "## load opencv haarcascade face from xml file\n",
    "haarcasecade_face_xml = './opencv/haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(haarcasecade_face_xml)\n",
    "\n",
    "## open camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        ## show frame with mirror effect\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        ## convert frame to gray\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        ## detect faces in frame\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        for (x, y, w, h) in faces:\n",
    "            ## draw rectangle around faces\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "            ## detect eyes in frame\n",
    "            eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "            ex_min = 0\n",
    "            ey_min = 0\n",
    "            ex_max = 0\n",
    "            ey_max = 0\n",
    "            for (ex, ey, ew, eh) in eyes:\n",
    "                ## draw rectangle around eyes\n",
    "                cv2.rectangle(frame, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "                if ex < ex_min:\n",
    "                    ex_min = ex\n",
    "                if ey < ey_min:\n",
    "                    ey_min = ey\n",
    "                if ex+ew > ex_max:\n",
    "                    ex_max = ex+ew\n",
    "                if ey+eh > ey_max:\n",
    "                    ey_max = ey+eh\n",
    "            ## draw rectangle around eyes\n",
    "            cv2.rectangle(frame, (ex_min, ey_min), (ex_max, ey_max), (0, 0, 255), 2)\n",
    "            \n",
    "        ## show frame\n",
    "        cv2.imshow('frame', frame)\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
