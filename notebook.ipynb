{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "## Outline\n",
    "1. Import Library\n",
    "2. Setup pre-requisites\n",
    "3. Extract dataset to images\n",
    "4. Upload images to Azure data store\n",
    "4. Setting up Azure ML Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /home/ntky/.local/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: torch in /home/ntky/.local/lib/python3.10/site-packages (2.0.1+cu117)\n",
      "Requirement already satisfied: torchvision in /home/ntky/.local/lib/python3.10/site-packages (0.15.2+cu117)\n",
      "Requirement already satisfied: torchaudio in /home/ntky/.local/lib/python3.10/site-packages (2.0.2+cu117)\n",
      "Collecting azureml-core\n",
      "  Downloading azureml_core-1.56.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /home/ntky/.local/lib/python3.10/site-packages (from opencv-python) (1.26.3)\n",
      "Requirement already satisfied: filelock in /home/ntky/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/ntky/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ntky/.local/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: sympy in /home/ntky/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ntky/.local/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ntky/.local/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: lit in /home/ntky/.local/lib/python3.10/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: cmake in /home/ntky/.local/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: requests in /home/ntky/.local/lib/python3.10/site-packages (from torchvision) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (9.0.1)\n",
      "Collecting jsonpickle<4.0.0\n",
      "  Downloading jsonpickle-3.2.2-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 KB\u001b[0m \u001b[31m713.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting contextlib2<22.0.0\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting knack<0.12.0\n",
      "  Downloading knack-0.11.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ndg-httpsclient<=0.5.1\n",
      "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
      "Collecting azure-mgmt-network<=26.0.0\n",
      "  Downloading azure_mgmt_network-25.4.0-py3-none-any.whl (614 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m614.8/614.8 KB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-mgmt-storage<=22.0.0,>=16.0.0\n",
      "  Downloading azure_mgmt_storage-21.2.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting azure-mgmt-resource<=24.0.0,>=15.0.0\n",
      "  Downloading azure_mgmt_resource-23.1.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting argcomplete<4\n",
      "  Downloading argcomplete-3.4.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathspec<1.0.0\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Collecting msrestazure<=0.6.4,>=0.4.33\n",
      "  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: SecretStorage<4.0.0 in /usr/lib/python3/dist-packages (from azureml-core) (3.3.1)\n",
      "Collecting pyopenssl<25.0.0\n",
      "  Downloading pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting msal<2.0.0,>=1.15.0\n",
      "  Downloading msal-1.30.0-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.8/111.8 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: paramiko<4.0.0,>=2.0.8 in /usr/lib/python3/dist-packages (from azureml-core) (2.9.3)\n",
      "Collecting urllib3<3.0.0,>1.26.17\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-common<2.0.0,>=1.1.12\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pytz in /usr/lib/python3/dist-packages (from azureml-core) (2022.1)\n",
      "Collecting jmespath<2.0.0\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting msal-extensions<=2.0.0,>=0.3.0\n",
      "  Downloading msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
      "Collecting adal<=1.2.7,>=1.2.0\n",
      "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-mgmt-authorization<5,>=0.40.0\n",
      "  Downloading azure_mgmt_authorization-4.0.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly<11.0,>=4.7\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker<8.0.0\n",
      "  Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting msrest<=0.7.1,>=0.5.1\n",
      "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-core<2.0.0\n",
      "  Downloading azure_core-1.30.2-py3-none-any.whl (194 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.3/194.3 KB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting backports.tempfile\n",
      "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting azure-mgmt-keyvault<11.0.0,>=0.40.0\n",
      "  Downloading azure_mgmt_keyvault-10.3.1-py3-none-any.whl (901 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.4/901.4 KB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging<=25.0,>=20.0 in /home/ntky/.local/lib/python3.10/site-packages (from azureml-core) (24.1)\n",
      "Collecting pkginfo\n",
      "  Downloading pkginfo-1.11.1-py3-none-any.whl (31 kB)\n",
      "Collecting azure-mgmt-containerregistry<11,>=8.2.0\n",
      "  Downloading azure_mgmt_containerregistry-10.3.0-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting azure-graphrbac<1.0.0,>=0.40.0\n",
      "  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.4/141.4 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /home/ntky/.local/lib/python3.10/site-packages (from azureml-core) (2.9.0.post0)\n",
      "Requirement already satisfied: PyJWT<3.0.0 in /usr/lib/python3/dist-packages (from azureml-core) (2.3.0)\n",
      "Requirement already satisfied: cryptography>=1.1.0 in /usr/lib/python3/dist-packages (from adal<=1.2.7,>=1.2.0->azureml-core) (3.4.8)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0->azureml-core) (1.16.0)\n",
      "Collecting isodate<1.0.0,>=0.6.1\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-mgmt-core<2.0.0,>=1.3.2\n",
      "  Downloading azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from knack<0.12.0->azureml-core) (5.4.1)\n",
      "Requirement already satisfied: pygments in /home/ntky/.local/lib/python3.10/site-packages (from knack<0.12.0->azureml-core) (2.18.0)\n",
      "Collecting portalocker<3,>=1.4\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from msrest<=0.7.1,>=0.5.1->azureml-core) (2020.6.20)\n",
      "Collecting pyasn1>=0.1.1\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=1.1.0\n",
      "  Downloading cryptography-43.0.0-cp39-abi3-manylinux_2_28_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ntky/.local/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ntky/.local/lib/python3.10/site-packages (from requests->torchvision) (1.7.1)\n",
      "Collecting backports.weakref\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ntky/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ntky/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ntky/.local/lib/python3.10/site-packages (from cryptography>=1.1.0->adal<=1.2.7,>=1.2.0->azureml-core) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.5.0->msrest<=0.7.1,>=0.5.1->azureml-core) (3.2.0)\n",
      "Requirement already satisfied: pycparser in /home/ntky/.local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.1.0->adal<=1.2.7,>=1.2.0->azureml-core) (2.22)\n",
      "Installing collected packages: backports.weakref, azure-common, urllib3, tabulate, pyasn1, portalocker, pkginfo, pathspec, jsonpickle, jmespath, isodate, humanfriendly, contextlib2, backports.tempfile, argcomplete, knack, cryptography, requests-oauthlib, pyopenssl, docker, azure-core, adal, ndg-httpsclient, msrest, msal, azure-mgmt-core, msrestazure, msal-extensions, azure-mgmt-storage, azure-mgmt-resource, azure-mgmt-network, azure-mgmt-keyvault, azure-mgmt-containerregistry, azure-mgmt-authorization, azure-graphrbac, azureml-core\n",
      "Successfully installed adal-1.2.7 argcomplete-3.4.0 azure-common-1.1.28 azure-core-1.30.2 azure-graphrbac-0.61.1 azure-mgmt-authorization-4.0.0 azure-mgmt-containerregistry-10.3.0 azure-mgmt-core-1.4.0 azure-mgmt-keyvault-10.3.1 azure-mgmt-network-25.4.0 azure-mgmt-resource-23.1.1 azure-mgmt-storage-21.2.1 azureml-core-1.56.0 backports.tempfile-1.0 backports.weakref-1.0.post1 contextlib2-21.6.0 cryptography-43.0.0 docker-7.1.0 humanfriendly-10.0 isodate-0.6.1 jmespath-1.0.1 jsonpickle-3.2.2 knack-0.11.0 msal-1.30.0 msal-extensions-1.2.0 msrest-0.7.1 msrestazure-0.6.4 ndg-httpsclient-0.5.1 pathspec-0.12.1 pkginfo-1.11.1 portalocker-2.10.1 pyasn1-0.6.0 pyopenssl-24.2.1 requests-oauthlib-2.0.0 tabulate-0.9.0 urllib3-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## Install needed packages\n",
    "%pip install opencv-python torch torchvision torchaudio azureml-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "import cv2\n",
    "# from azureml.core import Workspace, Dataset, Datastore\n",
    "\n",
    "## Using torchvision to create a dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone source code from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-22 19:33:20--  https://github.com/nnvu-fit/iusai-project/archive/refs/heads/main.zip\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/nnvu-fit/iusai-project/zip/refs/heads/main [following]\n",
      "--2024-07-22 19:33:21--  https://codeload.github.com/nnvu-fit/iusai-project/zip/refs/heads/main\n",
      "Resolving codeload.github.com (codeload.github.com)... 20.205.243.165\n",
      "Connecting to codeload.github.com (codeload.github.com)|20.205.243.165|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘main.zip’\n",
      "\n",
      "main.zip                [  <=>               ] 307,35K  1,46MB/s    in 0,2s    \n",
      "\n",
      "2024-07-22 19:33:21 (1,46 MB/s) - ‘main.zip’ saved [314727]\n",
      "\n",
      "Archive:  main.zip\n",
      "30c6d623a5535529b640fc32926bb83022b6ba2c\n",
      "   creating: iusai-project-main/\n",
      "   creating: iusai-project-main/.devcontainer/\n",
      "  inflating: iusai-project-main/.devcontainer/devcontainer.json  \n",
      "  inflating: iusai-project-main/.gitignore  \n",
      "   creating: iusai-project-main/.vscode/\n",
      "  inflating: iusai-project-main/.vscode/launch.json  \n",
      "  inflating: iusai-project-main/.vscode/settings.json  \n",
      "  inflating: iusai-project-main/Classification.ipynb  \n",
      "  inflating: iusai-project-main/Dockerfile  \n",
      " extracting: iusai-project-main/README.md  \n",
      "  inflating: iusai-project-main/config.json  \n",
      "  inflating: iusai-project-main/data_set.py  \n",
      "  inflating: iusai-project-main/image_classification_online.ipynb  \n",
      "  inflating: iusai-project-main/model.py  \n",
      "  inflating: iusai-project-main/notebook.ipynb  \n",
      "  inflating: iusai-project-main/requirements.txt  \n",
      "   creating: iusai-project-main/terraform/\n",
      " extracting: iusai-project-main/terraform/main.tf  \n",
      " extracting: iusai-project-main/terraform/provider.tf  \n",
      "   creating: iusai-project-main/the_app/\n",
      "   creating: iusai-project-main/the_app/helper/\n",
      "  inflating: iusai-project-main/the_app/helper/image_helper.py  \n",
      "  inflating: iusai-project-main/the_app/helper/model_helper.py  \n",
      "   creating: iusai-project-main/the_app/pages/\n",
      "  inflating: iusai-project-main/the_app/pages/classification.py  \n",
      "  inflating: iusai-project-main/the_app/streamlit_app.py  \n",
      "  inflating: iusai-project-main/train.py  \n",
      "  inflating: iusai-project-main/v2i.py  \n"
     ]
    }
   ],
   "source": [
    "## curl source code from github\n",
    "!wget 'https://github.com/nnvu-fit/iusai-project/archive/refs/heads/main.zip' -O main.zip\n",
    "!unzip main.zip\n",
    "!mv iusai-project-main/* .\n",
    "!rm -rf iusai-project-main main.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "ename": "UserErrorException",
     "evalue": "UserErrorException:\n\tMessage: You are currently logged-in to 66bbbbf2-d789-4231-8e48-08bfeb9cba42 tenant. You don't have access to d554f489-6933-4c33-8722-a536b3682bd7 subscription, please check if it is in this tenant. All the subscriptions that you have access to in this tenant are = \n [SubscriptionInfo(subscription_name='DEKRA_Application_Subscription', subscription_id='8db25668-388f-4e79-883a-54133883a597')]. \n Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"You are currently logged-in to 66bbbbf2-d789-4231-8e48-08bfeb9cba42 tenant. You don't have access to d554f489-6933-4c33-8722-a536b3682bd7 subscription, please check if it is in this tenant. All the subscriptions that you have access to in this tenant are = \\n [SubscriptionInfo(subscription_name='DEKRA_Application_Subscription', subscription_id='8db25668-388f-4e79-883a-54133883a597')]. \\n Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUserErrorException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Setup Azure ML Workspace\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ws \u001b[38;5;241m=\u001b[39m \u001b[43mWorkspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m## From workspace, get/create the default datastore\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m ws\u001b[38;5;241m.\u001b[39mget_default_datastore()\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\azureml\\core\\workspace.py:292\u001b[0m, in \u001b[0;36mWorkspace.from_config\u001b[1;34m(path, auth, _logger, _file_name)\u001b[0m\n\u001b[0;32m    288\u001b[0m subscription_id, resource_group, workspace_name \u001b[38;5;241m=\u001b[39m project_info\u001b[38;5;241m.\u001b[39mget_workspace_info(\n\u001b[0;32m    289\u001b[0m     found_path)\n\u001b[0;32m    291\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound the config file in: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, found_path)\n\u001b[1;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWorkspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubscription_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubscription_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresource_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_group\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\azureml\\core\\workspace.py:607\u001b[0m, in \u001b[0;36mWorkspace.get\u001b[1;34m(name, auth, subscription_id, resource_group, location, cloud, id)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m auth:\n\u001b[0;32m    605\u001b[0m     auth \u001b[38;5;241m=\u001b[39m InteractiveLoginAuthentication()\n\u001b[1;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWorkspace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubscription_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_cloud\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcloud\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_workspace_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\azureml\\core\\workspace.py:205\u001b[0m, in \u001b[0;36mWorkspace.__init__\u001b[1;34m(self, subscription_id, resource_group, workspace_name, auth, _location, _disable_service_check, _workspace_id, sku, tags, _cloud)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workspace_autorest_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _disable_service_check:\n\u001b[1;32m--> 205\u001b[0m     auto_rest_workspace \u001b[38;5;241m=\u001b[39m \u001b[43m_commands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_workspace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubscription_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_cloud\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_workspace_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workspace_autorest_object \u001b[38;5;241m=\u001b[39m auto_rest_workspace\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\azureml\\_project\\_commands.py:467\u001b[0m, in \u001b[0;36mget_workspace\u001b[1;34m(auth, subscription_id, resource_group_name, workspace_name, location, cloud, workspace_id)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m         workspaces \u001b[38;5;241m=\u001b[39m \u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_service_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m            \u001b[49m\u001b[43mAzureMachineLearningWorkspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubscription_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mworkspaces\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m WorkspacesOperations\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    471\u001b[0m             workspaces,\n\u001b[0;32m    472\u001b[0m             resource_group_name,\n\u001b[0;32m    473\u001b[0m             workspace_name)\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ClientRequestError:\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;66;03m# this most likely is because of failure to talk to arm.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;66;03m# Provide error message to user if they are in synapse enviroment.\u001b[39;00m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;66;03m# Additional params will enable aml dataplane get workspace call. \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\azureml\\core\\authentication.py:230\u001b[0m, in \u001b[0;36mAbstractAuthentication._get_service_client\u001b[1;34m(self, client_class, subscription_id, subscription_bound, base_url, is_check_subscription)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subscription_id \u001b[38;5;129;01mand\u001b[39;00m is_check_subscription:\n\u001b[0;32m    229\u001b[0m     all_subscription_list, tenant_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_all_subscription_ids()\n\u001b[1;32m--> 230\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_if_subscription_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubscription_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_subscription_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtenant_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m base_url:\n\u001b[0;32m    233\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cloud_type\u001b[38;5;241m.\u001b[39mendpoints\u001b[38;5;241m.\u001b[39mresource_manager\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\azureml\\core\\authentication.py:681\u001b[0m, in \u001b[0;36mInteractiveLoginAuthentication._check_if_subscription_exists\u001b[1;34m(self, subscription_id, subscription_id_list, tenant_id)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_if_subscription_exists\u001b[39m(\u001b[38;5;28mself\u001b[39m, subscription_id, subscription_id_list, tenant_id):\n\u001b[1;32m--> 681\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mInteractiveLoginAuthentication\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_if_subscription_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubscription_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43msubscription_id_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtenant_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\azureml\\core\\authentication.py:338\u001b[0m, in \u001b[0;36mAbstractAuthentication._check_if_subscription_exists\u001b[1;34m(self, subscription_id, subscription_id_list, tenant_id)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UserErrorException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you have specified subscription name, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, instead of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscription id. Subscription names may not be unique, please specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscription id from this list \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(subscription_id,\n\u001b[0;32m    336\u001b[0m                                                                            subscription_id_list))\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UserErrorException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are currently logged-in to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m tenant. You don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have access \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m subscription, please check if it is in this tenant. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll the subscriptions that you have access to in this tenant are = \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Please refer to aka.ms/aml-notebook-auth for different \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthentication mechanisms in azureml-sdk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(tenant_id,\n\u001b[0;32m    343\u001b[0m                                                                                 subscription_id,\n\u001b[0;32m    344\u001b[0m                                                                                 subscription_id_list))\n",
      "\u001b[1;31mUserErrorException\u001b[0m: UserErrorException:\n\tMessage: You are currently logged-in to 66bbbbf2-d789-4231-8e48-08bfeb9cba42 tenant. You don't have access to d554f489-6933-4c33-8722-a536b3682bd7 subscription, please check if it is in this tenant. All the subscriptions that you have access to in this tenant are = \n [SubscriptionInfo(subscription_name='DEKRA_Application_Subscription', subscription_id='8db25668-388f-4e79-883a-54133883a597')]. \n Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"You are currently logged-in to 66bbbbf2-d789-4231-8e48-08bfeb9cba42 tenant. You don't have access to d554f489-6933-4c33-8722-a536b3682bd7 subscription, please check if it is in this tenant. All the subscriptions that you have access to in this tenant are = \\n [SubscriptionInfo(subscription_name='DEKRA_Application_Subscription', subscription_id='8db25668-388f-4e79-883a-54133883a597')]. \\n Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "## Setup Azure ML Workspace\n",
    "ws = Workspace.from_config()\n",
    "## From workspace, get/create the default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "ws, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract dataset to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_path:  videos/\n",
      "images_path:  subjects-small/\n"
     ]
    }
   ],
   "source": [
    "## define videos location + images output location\n",
    "video_path = 'videos/'\n",
    "images_path = 'subjects-small/'\n",
    "print('video_path: ', video_path)\n",
    "print('images_path: ', images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label dict of videos/subject11/alhashe31.avi (start): [['0', 0], ['2', 0]]\n",
      "Label dict of videos/subject13/guolingh1.avi (start): [['0', 0], ['6', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject10/huangpi21.avi (start): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject12/makiluke1.avi (start): [['0', 0], ['2', 0]]\n",
      "Label dict of videos/subject15/bajajpak1.avi (start): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject14/chuangy61.avi (start): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject1/Yousef1.avi (start): [['0', 0], ['1', 0], ['2', 0], ['5', 0], ['6', 0], ['3', 0]]\n",
      "Label dict of videos/subject16/dhingra51.avi (start): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject17/zhoulian1.avi (start): [['0', 0], ['1', 0], ['2', 0]]\n",
      "Label dict of videos/subject21/Mark1.avi (start): [['0', 0], ['1', 0], ['2', 0]]\n",
      "Label dict of videos/subject2/Jourabloo1.avi (start): [['0', 0], ['6', 0], ['1', 0], ['2', 0], ['3', 0]]\n",
      "Label dict of videos/subject19/wangyus11.avi (start): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject20/Cool1.avi (start): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject18/yuqianyi1.avi (start): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject22/yarub1.avi (start): [['0', 0], ['1', 0], ['2', 0]]\n",
      "Label dict of videos/subject23/vahid1.avi (start): [['0', 0], ['1', 0], ['2', 0]]\n",
      "Label dict of videos/subject5/Muath1.avi (start): [['0', 0], ['1', 0], ['2', 0], ['6', 0], ['3', 0]]\n",
      "Label dict of videos/subject4/meowseph1.avi (start): [['0', 0], ['1', 0], ['2', 0], ['3', 0]]\n",
      "Label dict of videos/subject3/XiYin1.avi (start): [['0', 0], ['1', 0], ['2', 0], ['6', 0], ['3', 0]]\n",
      "Label dict of videos/subject24/mustaffa1.avi (start): [['0', 0], ['1', 0], ['5', 0], ['2', 0]]\n",
      "Label dict of videos/subject6/husseinhijazi1.avi (start): [['0', 0], ['2', 0], ['1', 0], ['5', 0], ['6', 0], ['3', 0]]\n",
      "Label dict of videos/subject9/chenlipi1.avi (start): [['0', 0], ['6', 0], ['1', 0], ['5', 0], ['3', 0]]\n",
      "Label dict of videos/subject8/vaibhav1.avi (start): [['0', 0], ['3', 0], ['1', 0], ['2', 0]]\n",
      "Label dict of videos/subject7/zach1.avi (start): [['0', 0], ['6', 0], ['1', 0], ['2', 0], ['3', 0]]\n",
      "Label dict of videos/subject1/Yousef1.avi (end): [['0', 0], ['1', 0], ['2', 0], ['5', 0], ['6', 0], ['3', 0]]\n",
      "Label dict of videos/subject3/XiYin1.avi (end): [['0', 0], ['1', 0], ['2', 0], ['6', 0], ['3', 0]]\n",
      "Label dict of videos/subject20/Cool1.avi (end): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject17/zhoulian1.avi (end): [['0', 0], ['1', 0], ['2', 0]]\n",
      "Label dict of videos/subject21/Mark1.avi (end): [['0', 0], ['1', 0], ['2', 0]]\n",
      "Label dict of videos/subject24/mustaffa1.avi (end): [['0', 0], ['1', 0], ['5', 0], ['2', 0]]\n",
      "Label dict of videos/subject22/yarub1.avi (end): [['0', 0], ['1', 0], ['2', 0]]\n",
      "Label dict of videos/subject7/zach1.avi (end): [['0', 0], ['6', 0], ['1', 0], ['2', 0], ['3', 0]]\n",
      "Label dict of videos/subject14/chuangy61.avi (end): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject5/Muath1.avi (end): [['0', 0], ['1', 0], ['2', 0], ['6', 0], ['3', 0]]\n",
      "Label dict of videos/subject6/husseinhijazi1.avi (end): [['0', 0], ['2', 0], ['1', 0], ['5', 0], ['6', 0], ['3', 0]]\n",
      "Label dict of videos/subject2/Jourabloo1.avi (end): [['0', 0], ['6', 0], ['1', 0], ['2', 0], ['3', 0]]\n",
      "Label dict of videos/subject4/meowseph1.avi (end): [['0', 0], ['1', 0], ['2', 0], ['3', 0]]\n",
      "Label dict of videos/subject11/alhashe31.avi (end): [['0', 0], ['2', 0]]\n",
      "Label dict of videos/subject16/dhingra51.avi (end): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject13/guolingh1.avi (end): [['0', 0], ['6', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject23/vahid1.avi (end): [['0', 0], ['1', 0], ['2', 0]]\n",
      "Label dict of videos/subject8/vaibhav1.avi (end): [['0', 0], ['3', 0], ['1', 0], ['2', 0]]\n",
      "Label dict of videos/subject15/bajajpak1.avi (end): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject9/chenlipi1.avi (end): [['0', 0], ['6', 0], ['1', 0], ['5', 0], ['3', 0]]\n",
      "Label dict of videos/subject18/yuqianyi1.avi (end): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject10/huangpi21.avi (end): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject19/wangyus11.avi (end): [['0', 0], ['2', 0], ['1', 0]]\n",
      "Label dict of videos/subject12/makiluke1.avi (end): [['0', 0], ['2', 0]]\n",
      "images_list_location_collection:  ['subject1', 'subject10', 'subject11', 'subject12', 'subject13', 'subject14', 'subject15', 'subject16', 'subject17', 'subject18', 'subject19', 'subject2', 'subject20', 'subject21', 'subject22', 'subject23', 'subject24', 'subject3', 'subject4', 'subject5', 'subject6', 'subject7', 'subject8', 'subject9']\n"
     ]
    }
   ],
   "source": [
    "import v2i\n",
    "reload(v2i)\n",
    "from v2i import extract_images_from_videos\n",
    "import threading\n",
    "\n",
    "def extract_images_from_videos_collection(video_path, images_path):\n",
    "    ## get all videos file in video_path\n",
    "    video_list_location_collection = os.listdir(video_path)\n",
    "\n",
    "    ## define total_label_dict\n",
    "    ## create threading pool\n",
    "    thread = []\n",
    "    ## for each video file\n",
    "    for video_list_location in video_list_location_collection:\n",
    "        ## check if video_location is not a directory (i.e. is a file), then skip\n",
    "        if not os.path.isdir(video_path + video_list_location):\n",
    "            continue\n",
    "        ## list videos in video_location\n",
    "        thread.append(threading.Thread(target=extract_images_from_videos, args=(video_path + video_list_location, images_path, 1)))\n",
    "        # extract_images_from_videos(video_path + video_list_location, images_path, inteval=1)\n",
    "\n",
    "    ## start all threads\n",
    "    for t in thread: t.start()\n",
    "    ## wait for all threads to finish\n",
    "    for t in thread:\n",
    "        t.join()\n",
    "    \n",
    "    ## check images in images_path\n",
    "    images_list_location_collection = os.listdir(images_path)\n",
    "    print('images_list_location_collection: ', images_list_location_collection)\n",
    "\n",
    "# video_subject_6_path = video_path + '/subject6-###'\n",
    "# ## list videos in video_location\n",
    "# label_dict = extract_images_from_videos(video_subject_6_path, images_path)\n",
    "\n",
    "## extract images from videos\n",
    "extract_images_from_videos_collection(video_path, images_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Upload images to Azure data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating arguments.\n",
      "Arguments validated.\n",
      "'overwrite' is set to True. Any file already present in the target will be overwritten.\n",
      "Uploading files from 'c:/Users/nnvuf/source/ai-final-project/images' to 'images-extra-small'\n",
      "Creating new dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', '/images-extra-small')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"22c174ee-e3e7-4e60-a949-e20eb237c73d\",\n",
       "    \"name\": \"images-extra-small\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"images-extra-small\",\n",
       "    \"workspace\": \"Workspace.create(name='ws-vunn-iusai-sea-sp6k7', subscription_id='d554f489-6933-4c33-8722-a536b3682bd7', resource_group='rg-vunn-iusai-sea-sp6k7')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## upload images to data asset using Dataset.File.upload_directory\n",
    "Dataset.File.upload_directory(src_dir=images_path, target=(ds, 'images-extra-small'), overwrite=True, show_progress=True) \\\n",
    "    .register(workspace=ws, name='images-extra-small', description='images-extra-small') ## register dataset \\\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Setup public workspace endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_collection:  images-extra-small  is already registered\n",
      "dataset_collection:  images-small-v2  is already registered\n",
      "dataset_collection:  images-medium  is already registered\n"
     ]
    }
   ],
   "source": [
    "dataset_collection_list = ['images-extra-small', 'images-small-v2', 'images-medium']\n",
    "## register dataset using Dataset.File.from_files\n",
    "for dataset_collection in dataset_collection_list:\n",
    "    ## check if dataset_collection is already registered\n",
    "    if dataset_collection in ws.datasets.keys():\n",
    "        print('dataset_collection: ', dataset_collection, ' is already registered')\n",
    "        continue\n",
    "    Dataset.File.from_files(path=(ds, dataset_collection)) \\\n",
    "        .register(workspace=ws, name=dataset_collection, description=dataset_collection) ## register dataset \\\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train models\n",
    "1. ResNET\n",
    "2. DenseNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup device + load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import train and scrore function from train.py\n",
    "import train as t;\n",
    "reload(t)\n",
    "from train import cross_validate, score_model, get_device\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_path is empty, download images_ds to images_path\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages_path is empty, download images_ds to images_path\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# download data asset to local\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m images_ds \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241m.\u001b[39mget_by_name(workspace\u001b[38;5;241m=\u001b[39mws, name\u001b[38;5;241m=\u001b[39mimages_ds_path)\n\u001b[1;32m     10\u001b[0m images_ds\u001b[38;5;241m.\u001b[39mdownload(target_path\u001b[38;5;241m=\u001b[39mimages_path, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "images_path = 'images'\n",
    "images_ds_path = \"images-extra-small\"\n",
    "if not os.path.exists(images_path):\n",
    "    os.mkdir(images_path)\n",
    "# download data asset to local if images_path is empty\n",
    "if len(os.listdir(images_path)) == 0:\n",
    "    print('images_path is empty, download images_ds to images_path')\n",
    "    # download data asset to local\n",
    "    images_ds = Dataset.get_by_name(workspace=ws, name=images_ds_path)\n",
    "    images_ds.download(target_path=images_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import data_set as ds\n",
    "reload(ds)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "imageDataset = ds.ImageDataset('images-small', transform=transform)\n",
    "## define batch_size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  [0, 1, 2, 3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "image, label = imageDataset.get_image(0)\n",
    "# image.show()\n",
    "labels = imageDataset.labels()\n",
    "## show labels in Interger\n",
    "print('labels: ',  [int(l) for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds:  18109\n",
      "val_ds:  4528\n",
      "test_ds:  2516\n"
     ]
    }
   ],
   "source": [
    "## split dataset into train and test dataset using random_split\n",
    "from torch.utils.data import random_split\n",
    "train_val_size = int(0.9 * len(imageDataset))\n",
    "test_size = len(imageDataset) - train_val_size\n",
    "train_val_ds, test_ds = random_split(imageDataset, [train_val_size, test_size])\n",
    "\n",
    "## split train_val_ds into train_ds and val_ds using random_split\n",
    "train_size = int(0.8 * len(train_val_ds))\n",
    "val_size = len(train_val_ds) - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(train_val_ds, [train_size, val_size])\n",
    "\n",
    "print('train_val_ds: ', len(train_val_ds))\n",
    "print('test_ds: ', len(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Resnes18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntky/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ntky/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/ntky/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|█████████████████| 44.7M/44.7M [00:00<00:00, 68.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " CrossEntropyLoss())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get resnet model of image classification from torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "## define optimizer using Adam and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.6718, Test Loss: 0.5714, Total Time: 00 hours, 01 minutes, 36 seconds\n",
      "Epoch 2/5, Train Loss: 0.4331, Test Loss: 0.3829, Total Time: 00 hours, 01 minutes, 31 seconds\n",
      "Epoch 3/5, Train Loss: 0.3102, Test Loss: 0.3836, Total Time: 00 hours, 01 minutes, 25 seconds\n",
      "Epoch 4/5, Train Loss: 0.2402, Test Loss: 0.3507, Total Time: 00 hours, 01 minutes, 29 seconds\n",
      "Epoch 5/5, Train Loss: 0.1827, Test Loss: 0.3296, Total Time: 00 hours, 01 minutes, 24 seconds\n",
      "Epoch 1/5, Train Loss: 0.1895, Test Loss: 0.1445, Total Time: 00 hours, 01 minutes, 24 seconds\n",
      "Epoch 2/5, Train Loss: 0.1357, Test Loss: 0.2599, Total Time: 00 hours, 01 minutes, 24 seconds\n",
      "Epoch 3/5, Train Loss: 0.1092, Test Loss: 0.1963, Total Time: 00 hours, 01 minutes, 25 seconds\n",
      "Epoch 4/5, Train Loss: 0.0906, Test Loss: 0.2349, Total Time: 00 hours, 01 minutes, 24 seconds\n",
      "Epoch 5/5, Train Loss: 0.0748, Test Loss: 0.2314, Total Time: 00 hours, 01 minutes, 24 seconds\n",
      "Epoch 1/5, Train Loss: 0.1227, Test Loss: 0.0936, Total Time: 00 hours, 01 minutes, 25 seconds\n",
      "Epoch 2/5, Train Loss: 0.0663, Test Loss: 0.0991, Total Time: 00 hours, 01 minutes, 28 seconds\n",
      "Epoch 3/5, Train Loss: 0.0561, Test Loss: 0.1124, Total Time: 00 hours, 01 minutes, 26 seconds\n",
      "Epoch 4/5, Train Loss: 0.0630, Test Loss: 0.1227, Total Time: 00 hours, 01 minutes, 25 seconds\n",
      "Epoch 5/5, Train Loss: 0.0538, Test Loss: 0.1429, Total Time: 00 hours, 01 minutes, 24 seconds\n",
      "Epoch 1/5, Train Loss: 0.0748, Test Loss: 0.0711, Total Time: 00 hours, 01 minutes, 25 seconds\n",
      "Epoch 2/5, Train Loss: 0.0478, Test Loss: 0.0628, Total Time: 00 hours, 01 minutes, 25 seconds\n",
      "Epoch 3/5, Train Loss: 0.0358, Test Loss: 0.0835, Total Time: 00 hours, 01 minutes, 24 seconds\n",
      "Epoch 4/5, Train Loss: 0.0511, Test Loss: 0.0904, Total Time: 00 hours, 01 minutes, 25 seconds\n",
      "Epoch 5/5, Train Loss: 0.0475, Test Loss: 0.0901, Total Time: 00 hours, 01 minutes, 25 seconds\n",
      "Epoch 1/5, Train Loss: 0.0578, Test Loss: 0.0357, Total Time: 00 hours, 01 minutes, 25 seconds\n",
      "Epoch 2/5, Train Loss: 0.0373, Test Loss: 0.0267, Total Time: 00 hours, 01 minutes, 26 seconds\n",
      "Epoch 3/5, Train Loss: 0.0361, Test Loss: 0.0415, Total Time: 00 hours, 01 minutes, 25 seconds\n",
      "Epoch 4/5, Train Loss: 0.0241, Test Loss: 0.0322, Total Time: 00 hours, 01 minutes, 26 seconds\n",
      "Epoch 5/5, Train Loss: 0.0328, Test Loss: 0.0665, Total Time: 00 hours, 01 minutes, 29 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5864619879564344"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train model\n",
    "reload(t)\n",
    "from train import cross_validate\n",
    "avg_lost = cross_validate(model, optimizer, loss_fn, train_val_ds, epochs=5, device=device)\n",
    "print('avg_lost: ', avg_lost)\n",
    "## score model\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## save model\n",
    "torch.save(model.state_dict(), 'model_resnes18.100_epochs.small.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Resnes34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " CrossEntropyLoss())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get resnet model of image classification from torchvision\n",
    "model = torchvision.models.resnet34(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "## define optimizer using Adam and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model using device (CPU or GPU) 10 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=1, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "torch.save(model.state_dict(), 'model_resnes34.small.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " CrossEntropyLoss())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## get resnet model of image classification from torchvision\n",
    "model = torchvision.models.densenet121(pretrained=True)\n",
    "## define optimizer using Adam and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\nnvuf\\source\\ai-final-project\\notebook.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nnvuf/source/ai-final-project/notebook.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## train model\u001b[39;00m\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nnvuf/source/ai-final-project/notebook.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice)\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nnvuf/source/ai-final-project/notebook.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m## score model\u001b[39;00m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nnvuf/source/ai-final-project/notebook.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m score_model(model, loss_fn, test_dataloader, device\u001b[39m=\u001b[39mdevice)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\nnvuf\\source\\ai-final-project\\train.py:35\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, train_dataset, test_dataset, epochs, device)\u001b[0m\n",
      "\u001b[0;32m     33\u001b[0m   loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;32m     34\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m---> 35\u001b[0m   train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;32m     36\u001b[0m train_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_dataset\u001b[39m.\u001b[39mdataset)\n",
      "\u001b[0;32m     37\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## train model\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=1, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4070, Test Loss: 0.4468, Total Time: 00 hours, 10 minutes, 50 seconds\n",
      "Epoch 2/10, Train Loss: 0.2953, Test Loss: 0.3624, Total Time: 00 hours, 12 minutes, 53 seconds\n",
      "Epoch 3/10, Train Loss: 0.2196, Test Loss: 0.3772, Total Time: 00 hours, 13 minutes, 49 seconds\n",
      "Epoch 4/10, Train Loss: 0.1600, Test Loss: 0.3560, Total Time: 00 hours, 16 minutes, 10 seconds\n",
      "Epoch 5/10, Train Loss: 0.1331, Test Loss: 0.3690, Total Time: 00 hours, 15 minutes, 54 seconds\n",
      "Epoch 6/10, Train Loss: 0.1037, Test Loss: 0.3158, Total Time: 00 hours, 15 minutes, 15 seconds\n",
      "Epoch 7/10, Train Loss: 0.0790, Test Loss: 0.3453, Total Time: 00 hours, 15 minutes, 53 seconds\n",
      "Epoch 8/10, Train Loss: 0.0761, Test Loss: 0.4047, Total Time: 00 hours, 15 minutes, 15 seconds\n",
      "Epoch 9/10, Train Loss: 0.0618, Test Loss: 0.3804, Total Time: 00 hours, 15 minutes, 21 seconds\n",
      "Epoch 10/10, Train Loss: 0.0561, Test Loss: 0.4284, Total Time: 00 hours, 16 minutes, 02 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4667215104080109"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## train model using device (CPU or GPU) 10 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=100, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "torch.save(model.state_dict(), 'model_resnes18.100_epochs.small.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nnvuf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " CrossEntropyLoss())"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get denseNet model of image classification from torchvision\n",
    "model = torchvision.models.densenet121(pretrained=True, num_classes=len(labels))\n",
    "## define optimizer using Adam and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Train Loss: 0.6834, Test Loss: 0.7046, Total Time: 05 hours, 27 minutes, 36 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7259557617677225"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train model using device (CPU or GPU) 1 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=1, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4411, Test Loss: 0.6275, Total Time: 05 hours, 34 minutes, 19 seconds\n",
      "Epoch 2/10, Train Loss: 0.3439, Test Loss: 0.6895, Total Time: 05 hours, 34 minutes, 47 seconds\n",
      "Epoch 3/10, Train Loss: 0.2888, Test Loss: 0.4157, Total Time: 05 hours, 43 minutes, 16 seconds\n",
      "Epoch 4/10, Train Loss: 0.2428, Test Loss: 0.4163, Total Time: 05 hours, 35 minutes, 47 seconds\n",
      "Epoch 5/10, Train Loss: 0.2049, Test Loss: 0.3158, Total Time: 07 hours, 43 minutes, 43 seconds\n",
      "Epoch 6/10, Train Loss: 0.1741, Test Loss: 0.5003, Total Time: 08 hours, 44 minutes, 26 seconds\n",
      "Epoch 7/10, Train Loss: 0.1587, Test Loss: 0.3826, Total Time: 08 hours, 34 minutes, 52 seconds\n",
      "Epoch 8/10, Train Loss: 0.1292, Test Loss: 0.3596, Total Time: 08 hours, 28 minutes, 02 seconds\n",
      "Epoch 9/10, Train Loss: 0.1110, Test Loss: 0.4189, Total Time: 17 hours, 53 minutes, 29 seconds\n",
      "Epoch 10/10, Train Loss: 0.1113, Test Loss: 0.3861, Total Time: 08 hours, 27 minutes, 54 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41211051089964534"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train model using device (CPU or GPU) 10 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=10, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "## save model\n",
    "torch.save(model.state_dict(), 'model_densenet121.small.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using Yolov5 to extract face then using ResNet to classification the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use yoloresnet model\n",
    "import model\n",
    "reload(model)\n",
    "from model import YoloResnet\n",
    "model = YoloResnet()\n",
    "## define optimizer using Adam and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model using device (CPU or GPU) 1 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=1, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model using device (CPU or GPU) 10 epoch\n",
    "train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs=10, device=device)\n",
    "## score model\n",
    "score_model(model, loss_fn, test_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "torch.save(model.state_dict(), 'model_yoloresnet.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
