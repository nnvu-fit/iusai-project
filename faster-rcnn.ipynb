{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "epoch:  0\n",
      "[{'id': tensor([49,  0,  4, 58,  3, 22, 53, 61, 45, 30, 51, 16, 23, 29, 62, 35]), 'image_id': tensor([49,  0,  4, 58,  3, 22, 53, 61, 45, 30, 51, 16, 23, 29, 62, 35]), 'category_id': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'bbox': [tensor([269, 316, 310, 292, 311, 257, 268, 289, 249, 247, 297, 294, 316, 296,\n",
      "        247, 289]), tensor([321, 340, 352, 282, 305, 285, 330, 286, 307, 289, 284, 308, 329, 356,\n",
      "        352, 281]), tensor([117.5000,  96.0000, 101.5000, 104.0000,  96.5000, 104.0000, 117.5000,\n",
      "        104.0000,  97.5000, 104.5000, 102.0000, 102.0000,  95.5000, 107.5000,\n",
      "        115.5000, 102.5000], dtype=torch.float64), tensor([83.0000, 86.5000, 82.0000, 85.5000, 98.5000, 74.5000, 70.0000, 82.0000,\n",
      "        77.0000, 79.5000, 84.0000, 79.5000, 86.5000, 86.5000, 83.0000, 87.5000])], 'area': tensor([9752.5000, 8304.0000, 8323.0000, 8892.0000, 9505.2500, 7748.0000,\n",
      "        8225.0000, 8528.0000, 7507.5000, 8307.7500, 8568.0000, 8109.0000,\n",
      "        8260.7500, 9298.7500, 9586.5000, 8968.7500], dtype=torch.float64), 'segmentation': [], 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nnvuf\\AppData\\Local\\Temp\\ipykernel_8588\\791779292.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target['labels'] = torch.stack([torch.tensor(category_id[i], dtype=torch.int64).to(device)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': tensor([ 9,  1, 20, 14, 17, 12, 41, 15, 44, 36, 38, 13, 31, 47, 24, 50]), 'image_id': tensor([ 9,  1, 20, 14, 17, 12, 41, 15, 44, 36, 38, 13, 31, 47, 24, 50]), 'category_id': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'bbox': [tensor([248, 300, 249, 289, 285, 246, 297, 290, 316, 307, 266, 295, 267, 247,\n",
      "        300, 288]), tensor([360, 285, 317, 358, 286, 337, 284, 281, 344, 352, 329, 290, 364, 343,\n",
      "        356, 284]), tensor([120.0000, 100.0000, 100.5000, 112.5000, 103.5000, 110.0000, 102.0000,\n",
      "        105.5000,  96.0000, 102.5000, 116.0000,  99.5000, 126.0000, 109.5000,\n",
      "        106.5000, 103.5000]), tensor([80.5000, 89.0000, 77.0000, 83.0000, 83.0000, 79.5000, 84.0000, 90.0000,\n",
      "        89.0000, 85.5000, 72.5000, 84.0000, 82.0000, 76.0000, 82.0000, 80.5000],\n",
      "       dtype=torch.float64)], 'area': tensor([ 9660.0000,  8900.0000,  7738.5000,  9337.5000,  8590.5000,  8745.0000,\n",
      "         8568.0000,  9495.0000,  8544.0000,  8763.7500,  8410.0000,  8358.0000,\n",
      "        10332.0000,  8322.0000,  8733.0000,  8331.7500]), 'segmentation': [], 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}]\n",
      "4\n",
      "[{'id': tensor([42,  5, 34, 21, 25, 33, 59, 26, 32, 46, 52, 10, 40, 19,  8, 11]), 'image_id': tensor([42,  5, 34, 21, 25, 33, 59, 26, 32, 46, 52, 10, 40, 19,  8, 11]), 'category_id': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'bbox': [tensor([262, 250, 304, 307, 312, 286, 245, 291, 269, 280, 269, 272, 248, 313,\n",
      "        289, 249]), tensor([363, 299, 293, 299, 315, 286, 330, 281, 331, 319, 325, 323, 301, 320,\n",
      "        300, 286]), tensor([124.5000,  96.0000, 102.0000,  99.5000,  98.0000, 104.5000, 109.5000,\n",
      "        100.5000, 116.0000, 112.0000, 115.5000, 112.0000,  98.5000,  97.5000,\n",
      "        105.5000, 108.5000], dtype=torch.float64), tensor([83.0000, 76.0000, 92.5000, 95.0000, 89.0000, 84.0000, 82.0000, 85.5000,\n",
      "        68.5000, 76.0000, 80.5000, 78.0000, 77.0000, 91.5000, 90.0000, 85.5000])], 'area': tensor([10333.5000,  7296.0000,  9435.0000,  9452.5000,  8722.0000,  8778.0000,\n",
      "         8979.0000,  8592.7500,  7946.0000,  8512.0000,  9297.7500,  8736.0000,\n",
      "         7584.5000,  8921.2500,  9495.0000,  9276.7500], dtype=torch.float64), 'segmentation': [], 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}]\n",
      "4\n",
      "[{'id': tensor([60,  7, 37, 56, 48, 39, 54, 18, 55, 28, 27,  2, 57,  6, 43]), 'image_id': tensor([60,  7, 37, 56, 48, 39, 54, 18, 55, 28, 27,  2, 57,  6, 43]), 'category_id': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'bbox': [tensor([288, 255, 278, 279, 279, 244, 248, 249, 264, 294, 249, 251, 265, 293,\n",
      "        293]), tensor([285, 365, 361, 314, 284, 350, 293, 323, 287, 298, 289, 365, 326, 300,\n",
      "        280]), tensor([104.0000, 123.5000, 117.5000, 109.5000, 111.5000, 116.0000, 100.5000,\n",
      "        102.5000, 108.5000, 101.5000, 105.5000, 125.5000, 118.0000, 106.0000,\n",
      "        104.0000]), tensor([84.0000, 82.0000, 83.0000, 83.0000, 77.0000, 80.5000, 80.5000, 78.0000,\n",
      "        67.5000, 84.0000, 80.5000, 80.5000, 79.5000, 83.0000, 87.5000])], 'area': tensor([ 8736.0000, 10127.0000,  9752.5000,  9088.5000,  8585.5000,  9338.0000,\n",
      "         8090.2500,  7995.0000,  7323.7500,  8526.0000,  8492.7500, 10102.7500,\n",
      "         9381.0000,  8798.0000,  9100.0000]), 'segmentation': [], 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}]\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (list, int), but expected one of:\n * (Tensor input, *, Tensor out = None)\n * (Tensor input, Tensor other, *, Tensor out = None)\n * (Tensor input, int dim, bool keepdim = False, *, tuple of Tensors out = None)\n * (Tensor input, name dim, bool keepdim = False, *, tuple of Tensors out = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m labels \u001b[38;5;241m=\u001b[39m get_labels(labels, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    100\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[1;32m--> 101\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    103\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (list, int), but expected one of:\n * (Tensor input, *, Tensor out = None)\n * (Tensor input, Tensor other, *, Tensor out = None)\n * (Tensor input, int dim, bool keepdim = False, *, tuple of Tensors out = None)\n * (Tensor input, name dim, bool keepdim = False, *, tuple of Tensors out = None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# get device to train on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define the network architecture based on ResNet18 as backbone\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "def get_labels(labels, device=None):\n",
    "    targets = []\n",
    "    bbox = labels[0]['bbox']\n",
    "    category_id = labels[0]['category_id']\n",
    "    print(len(bbox))\n",
    "    for i in range(len(bbox[0])):\n",
    "        target = {}\n",
    "        xmin, ymin, width, height = [bbox[0][i], bbox[1][i], bbox[2][i], bbox[3][i]]\n",
    "        xmax = xmin + width\n",
    "        ymax = ymin + height\n",
    "        target['boxes'] = torch.stack([torch.tensor([xmin, ymin, xmax, ymax], dtype=torch.float32).to(device)])\n",
    "        target['labels'] = torch.stack([torch.tensor(category_id[i], dtype=torch.int64).to(device)])\n",
    "        targets.append(target)\n",
    "    return targets\n",
    "\n",
    "# define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define the optimizer\n",
    "net = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "net = net.to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# load the data\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((640, 640)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# dataset_path = './dataset/FasterRCNN/'\n",
    "dataset_path = './datasets/FasterRCNN/data/'\n",
    "# coco dataset\n",
    "train_set = torchvision.datasets.CocoDetection(\n",
    "    root=dataset_path+'train',\n",
    "    annFile=dataset_path+'train/_annotations.coco.json',\n",
    "    transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_set = torchvision.datasets.CocoDetection(\n",
    "    root=dataset_path+'valid',\n",
    "    annFile=dataset_path+'valid/_annotations.coco.json',\n",
    "    transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "# train the network\n",
    "for epoch in range(1):\n",
    "    print('epoch: ', epoch)\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        print(labels)\n",
    "        labels = get_labels(labels, device=device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_dict = net(inputs, labels)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = get_labels(labels, device=device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the test images: %d %%' % ( 100 * correct / total))\n",
    "\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "# save the model\n",
    "torch.save(net.state_dict(), './model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
